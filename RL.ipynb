{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEu-hfh7JDWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THgOwTNUJFR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "## Loading the environment\n",
        "I am running the RL algorithms on LunarLander-v3 , which is one of many gym environments. The goal of the game is to safely land a rover on a planet surface. The espisode ends after the rover touches the ground and a reward is given, which is calculated based on the time taken to land, the smoothness of the landing and potential damages to the rover. The action space consists of 4 (one-hot) actions and the observation space of 8 continuous measurements."
      ],
      "metadata": {
        "id": "cMW6fvVDJEkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! apt-get install swig3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoKSn7kIJmrg",
        "outputId": "01f78f36-78fa-4057-f955-98afa46b0d33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (1,567 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ln -s /usr/bin/swig3.0 /usr/bin/swig"
      ],
      "metadata": {
        "id": "zO0qwrucJ6CZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMkTdbyMJ7ze",
        "outputId": "9158cd2c-d398-49db-a90b-308a24976f45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2312070 sha256=731a48864dad0f720f54267b19821af5f477779d9421ab5da268bd553b02307f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\")"
      ],
      "metadata": {
        "id": "_kf6FdLsJCMn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q Learning\n",
        "In the first part of notebook, I am working with value-based DRL algorithms, namely DQN. I am using a simple feedforward NN with architecture 8-128-64-4 and ReLU for activation.\n",
        "\n",
        "### Baseline Deep Q Learning\n",
        "I am starting with a basic Q Learning procedure. Namely, I run the model on every step of the simulation, selecting the action with the highest estimated value (ie. $ \\arg\\max_{a} Q(s, a) $ ), then transition to the next state on the environemnt $s'$ and optimize using the the MSQE loss function:\n",
        "$$ \\lVert \\arg\\max_{a'} Q(s', a') - ( R + γ Q(s, a) ) \\rVert ^2 $$\n",
        "\n",
        "I am also incorporating an $\\epsilon$-greedy stategy of expoloration, in which case the algorithm a uniformly random action, ignoring the model outputs.\n"
      ],
      "metadata": {
        "id": "5ldBccgSKXGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "sF2DTnk_XJEo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Uv6wXpI_JgHe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dql_baseline(model, optimizer, num_episodes, gamma, epsilon, device):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        observation, info = env.reset()\n",
        "        observation = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)  # (1, obs_state_dim)\n",
        "\n",
        "        action = 0 # start with a dummy action\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_reward = 0.0\n",
        "        num_runs = 0\n",
        "\n",
        "        while True:\n",
        "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            next_observation = torch.tensor(next_observation, dtype=torch.float32, device=device).unsqueeze(0) # (1, obs_state_dim)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            observations_bundled = torch.cat([observation, next_observation], dim=0)  # (2, obs_state_dim)\n",
        "            q_values = model(observations_bundled)  # (2, num_actions)\n",
        "\n",
        "            best_values = q_values.max(dim=1).values  # (prev_value, next_value)\n",
        "\n",
        "            loss = criterion(best_values[0], reward + gamma * best_values[1].detach())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_runs += 1\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = q_values.argmax(dim=1)[1].item()  # highest value on the 2nd prediction (next_value)\n",
        "\n",
        "        avg_loss = total_loss / num_runs if num_runs > 0 else 0\n",
        "        avg_reward = total_reward / num_runs if num_runs > 0 else 0\n",
        "\n",
        "        print(f'Episode {episode}   Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.2f}')\n",
        "\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "KeiqB1QhKTjv"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "T1V10h-ZZ9VU"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 300\n",
        "gamma = 0.99\n",
        "epsilon = 0.05\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "model = DQN(obs_state_dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "dql_baseline(model, optimizer, num_episodes, gamma, epsilon, device)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8E5abtsKT9u",
        "outputId": "e78750f5-5e69-43de-af39-77706e5e369e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0   Avg Loss: 36.7307, Avg Reward: -2.36\n",
            "Episode 1   Avg Loss: 22.8168, Avg Reward: -4.43\n",
            "Episode 2   Avg Loss: 180.7674, Avg Reward: -4.24\n",
            "Episode 3   Avg Loss: 283.7177, Avg Reward: -7.18\n",
            "Episode 4   Avg Loss: 52.9152, Avg Reward: -2.36\n",
            "Episode 5   Avg Loss: 728.9474, Avg Reward: -9.44\n",
            "Episode 6   Avg Loss: 67.7815, Avg Reward: -8.02\n",
            "Episode 7   Avg Loss: 47.8840, Avg Reward: -8.06\n",
            "Episode 8   Avg Loss: 3.7803, Avg Reward: -9.05\n",
            "Episode 9   Avg Loss: 14.8400, Avg Reward: -6.98\n",
            "Episode 10   Avg Loss: 2.8627, Avg Reward: -8.43\n",
            "Episode 11   Avg Loss: 2.8828, Avg Reward: -7.58\n",
            "Episode 12   Avg Loss: 1.3779, Avg Reward: -6.98\n",
            "Episode 13   Avg Loss: 4.8145, Avg Reward: -7.95\n",
            "Episode 14   Avg Loss: 3.8329, Avg Reward: -6.73\n",
            "Episode 15   Avg Loss: 186.9174, Avg Reward: -6.35\n",
            "Episode 16   Avg Loss: 10.0212, Avg Reward: -8.91\n",
            "Episode 17   Avg Loss: 3.4724, Avg Reward: -8.38\n",
            "Episode 18   Avg Loss: 2.2988, Avg Reward: -8.39\n",
            "Episode 19   Avg Loss: 5.5683, Avg Reward: -9.95\n",
            "Episode 20   Avg Loss: 7.7069, Avg Reward: -7.72\n",
            "Episode 21   Avg Loss: 4.9836, Avg Reward: -5.71\n",
            "Episode 22   Avg Loss: 7.7021, Avg Reward: -8.76\n",
            "Episode 23   Avg Loss: 10.0925, Avg Reward: -9.33\n",
            "Episode 24   Avg Loss: 3.8104, Avg Reward: -8.95\n",
            "Episode 25   Avg Loss: 205.3713, Avg Reward: -5.62\n",
            "Episode 26   Avg Loss: 28.2431, Avg Reward: -9.15\n",
            "Episode 27   Avg Loss: 4.9013, Avg Reward: -9.57\n",
            "Episode 28   Avg Loss: 3.8231, Avg Reward: -8.46\n",
            "Episode 29   Avg Loss: 1.3900, Avg Reward: -8.67\n",
            "Episode 30   Avg Loss: 1.4485, Avg Reward: -7.53\n",
            "Episode 31   Avg Loss: 4.8143, Avg Reward: -5.81\n",
            "Episode 32   Avg Loss: 10.5452, Avg Reward: -8.76\n",
            "Episode 33   Avg Loss: 3.9989, Avg Reward: -8.08\n",
            "Episode 34   Avg Loss: 2.1465, Avg Reward: -7.47\n",
            "Episode 35   Avg Loss: 26.1969, Avg Reward: -9.87\n",
            "Episode 36   Avg Loss: 13.7539, Avg Reward: -7.86\n",
            "Episode 37   Avg Loss: 8.8787, Avg Reward: -8.82\n",
            "Episode 38   Avg Loss: 103.8914, Avg Reward: -11.43\n",
            "Episode 39   Avg Loss: 73.9443, Avg Reward: -8.34\n",
            "Episode 40   Avg Loss: 62.4195, Avg Reward: -9.86\n",
            "Episode 41   Avg Loss: 222.0543, Avg Reward: -3.59\n",
            "Episode 42   Avg Loss: 83.5613, Avg Reward: -6.20\n",
            "Episode 43   Avg Loss: 151.3437, Avg Reward: -7.06\n",
            "Episode 44   Avg Loss: 53.6686, Avg Reward: -2.42\n",
            "Episode 45   Avg Loss: 2.6565, Avg Reward: -2.16\n",
            "Episode 46   Avg Loss: 23.8242, Avg Reward: -2.64\n",
            "Episode 47   Avg Loss: 1.7771, Avg Reward: -2.25\n",
            "Episode 48   Avg Loss: 1.1251, Avg Reward: -1.88\n",
            "Episode 49   Avg Loss: 2.7553, Avg Reward: -2.35\n",
            "Episode 50   Avg Loss: 1.9541, Avg Reward: -2.03\n",
            "Episode 51   Avg Loss: 214.8534, Avg Reward: 0.14\n",
            "Episode 52   Avg Loss: 2.5343, Avg Reward: -1.98\n",
            "Episode 53   Avg Loss: 5.5585, Avg Reward: -1.71\n",
            "Episode 54   Avg Loss: 2.1779, Avg Reward: -2.58\n",
            "Episode 55   Avg Loss: 172.2570, Avg Reward: -0.15\n",
            "Episode 56   Avg Loss: 2.2434, Avg Reward: -2.18\n",
            "Episode 57   Avg Loss: 1.9211, Avg Reward: -1.35\n",
            "Episode 58   Avg Loss: 1.9201, Avg Reward: -1.81\n",
            "Episode 59   Avg Loss: 1.9253, Avg Reward: -2.12\n",
            "Episode 60   Avg Loss: 2.4175, Avg Reward: -2.62\n",
            "Episode 61   Avg Loss: 1.8005, Avg Reward: -2.08\n",
            "Episode 62   Avg Loss: 280.2699, Avg Reward: -0.11\n",
            "Episode 63   Avg Loss: 19.2611, Avg Reward: -1.98\n",
            "Episode 64   Avg Loss: 6.3601, Avg Reward: -1.68\n",
            "Episode 65   Avg Loss: 2.4628, Avg Reward: -1.66\n",
            "Episode 66   Avg Loss: 2.9298, Avg Reward: -1.79\n",
            "Episode 67   Avg Loss: 3.4613, Avg Reward: -2.47\n",
            "Episode 68   Avg Loss: 7.9132, Avg Reward: -1.39\n",
            "Episode 69   Avg Loss: 1.7837, Avg Reward: -1.84\n",
            "Episode 70   Avg Loss: 117.7937, Avg Reward: -0.60\n",
            "Episode 71   Avg Loss: 3.9854, Avg Reward: -1.75\n",
            "Episode 72   Avg Loss: 51.1755, Avg Reward: -1.95\n",
            "Episode 73   Avg Loss: 4.2639, Avg Reward: -1.75\n",
            "Episode 74   Avg Loss: 38.2223, Avg Reward: -1.57\n",
            "Episode 75   Avg Loss: 2.2569, Avg Reward: -2.31\n",
            "Episode 76   Avg Loss: 4.6862, Avg Reward: -2.40\n",
            "Episode 77   Avg Loss: 2.0838, Avg Reward: -2.22\n",
            "Episode 78   Avg Loss: 2.0134, Avg Reward: -2.56\n",
            "Episode 79   Avg Loss: 1.4565, Avg Reward: -2.01\n",
            "Episode 80   Avg Loss: 3.4165, Avg Reward: -2.02\n",
            "Episode 81   Avg Loss: 3.6273, Avg Reward: -2.55\n",
            "Episode 82   Avg Loss: 6.1762, Avg Reward: -1.84\n",
            "Episode 83   Avg Loss: 2.0413, Avg Reward: -1.36\n",
            "Episode 84   Avg Loss: 1.6364, Avg Reward: -1.22\n",
            "Episode 85   Avg Loss: 2.2413, Avg Reward: -2.70\n",
            "Episode 86   Avg Loss: 5.7190, Avg Reward: -1.85\n",
            "Episode 87   Avg Loss: 5.5008, Avg Reward: -1.80\n",
            "Episode 88   Avg Loss: 2.0272, Avg Reward: -2.81\n",
            "Episode 89   Avg Loss: 1.8347, Avg Reward: -2.42\n",
            "Episode 90   Avg Loss: 92.3444, Avg Reward: -1.00\n",
            "Episode 91   Avg Loss: 3.7666, Avg Reward: -2.44\n",
            "Episode 92   Avg Loss: 2.7419, Avg Reward: -2.21\n",
            "Episode 93   Avg Loss: 6.3645, Avg Reward: -1.69\n",
            "Episode 94   Avg Loss: 1.6740, Avg Reward: -2.72\n",
            "Episode 95   Avg Loss: 4.2803, Avg Reward: -1.66\n",
            "Episode 96   Avg Loss: 1.2125, Avg Reward: -1.37\n",
            "Episode 97   Avg Loss: 1.6794, Avg Reward: -2.09\n",
            "Episode 98   Avg Loss: 2.4076, Avg Reward: -1.44\n",
            "Episode 99   Avg Loss: 1.7391, Avg Reward: -2.77\n",
            "Episode 100   Avg Loss: 1.4161, Avg Reward: -1.60\n",
            "Episode 101   Avg Loss: 6.2117, Avg Reward: -2.64\n",
            "Episode 102   Avg Loss: 4.0833, Avg Reward: -1.23\n",
            "Episode 103   Avg Loss: 27.3003, Avg Reward: -1.13\n",
            "Episode 104   Avg Loss: 2.7904, Avg Reward: -2.04\n",
            "Episode 105   Avg Loss: 1.4605, Avg Reward: -1.66\n",
            "Episode 106   Avg Loss: 1.5821, Avg Reward: -1.91\n",
            "Episode 107   Avg Loss: 1.7260, Avg Reward: -1.96\n",
            "Episode 108   Avg Loss: 3.7242, Avg Reward: -2.37\n",
            "Episode 109   Avg Loss: 2.2144, Avg Reward: -2.46\n",
            "Episode 110   Avg Loss: 5.6564, Avg Reward: -1.90\n",
            "Episode 111   Avg Loss: 4.1394, Avg Reward: -2.24\n",
            "Episode 112   Avg Loss: 2.7796, Avg Reward: -1.69\n",
            "Episode 113   Avg Loss: 5.4635, Avg Reward: -1.74\n",
            "Episode 114   Avg Loss: 1.2373, Avg Reward: -1.79\n",
            "Episode 115   Avg Loss: 13.6473, Avg Reward: -1.86\n",
            "Episode 116   Avg Loss: 2.4355, Avg Reward: -2.15\n",
            "Episode 117   Avg Loss: 5.5729, Avg Reward: -1.11\n",
            "Episode 118   Avg Loss: 11.2769, Avg Reward: -1.63\n",
            "Episode 119   Avg Loss: 8.4800, Avg Reward: -1.51\n",
            "Episode 120   Avg Loss: 5.7986, Avg Reward: -1.91\n",
            "Episode 121   Avg Loss: 3.3226, Avg Reward: -2.20\n",
            "Episode 122   Avg Loss: 2.4544, Avg Reward: -2.01\n",
            "Episode 123   Avg Loss: 2.2679, Avg Reward: -1.24\n",
            "Episode 124   Avg Loss: 5.5520, Avg Reward: -2.02\n",
            "Episode 125   Avg Loss: 2.1433, Avg Reward: -1.72\n",
            "Episode 126   Avg Loss: 2.5249, Avg Reward: -1.72\n",
            "Episode 127   Avg Loss: 4.5229, Avg Reward: -1.10\n",
            "Episode 128   Avg Loss: 4.7412, Avg Reward: -1.71\n",
            "Episode 129   Avg Loss: 3.6654, Avg Reward: -1.63\n",
            "Episode 130   Avg Loss: 6.1486, Avg Reward: -1.76\n",
            "Episode 131   Avg Loss: 2.1306, Avg Reward: -2.72\n",
            "Episode 132   Avg Loss: 1.7045, Avg Reward: -1.53\n",
            "Episode 133   Avg Loss: 2.7111, Avg Reward: -1.46\n",
            "Episode 134   Avg Loss: 6.6701, Avg Reward: -1.69\n",
            "Episode 135   Avg Loss: 2.2697, Avg Reward: -2.42\n",
            "Episode 136   Avg Loss: 1.3924, Avg Reward: -2.45\n",
            "Episode 137   Avg Loss: 1.5877, Avg Reward: -2.61\n",
            "Episode 138   Avg Loss: 1.7632, Avg Reward: -2.05\n",
            "Episode 139   Avg Loss: 1.9753, Avg Reward: -2.38\n",
            "Episode 140   Avg Loss: 24.0260, Avg Reward: -1.23\n",
            "Episode 141   Avg Loss: 6.6722, Avg Reward: -1.81\n",
            "Episode 142   Avg Loss: 281.2485, Avg Reward: -3.71\n",
            "Episode 143   Avg Loss: 2.9398, Avg Reward: -1.56\n",
            "Episode 144   Avg Loss: 9.5757, Avg Reward: -1.88\n",
            "Episode 145   Avg Loss: 3.1864, Avg Reward: -2.32\n",
            "Episode 146   Avg Loss: 3.7490, Avg Reward: -1.50\n",
            "Episode 147   Avg Loss: 2.2836, Avg Reward: -1.69\n",
            "Episode 148   Avg Loss: 6.2799, Avg Reward: -2.27\n",
            "Episode 149   Avg Loss: 6.5500, Avg Reward: -1.56\n",
            "Episode 150   Avg Loss: 1.7838, Avg Reward: -1.87\n",
            "Episode 151   Avg Loss: 4.1288, Avg Reward: -2.05\n",
            "Episode 152   Avg Loss: 284.1654, Avg Reward: 0.28\n",
            "Episode 153   Avg Loss: 92.5183, Avg Reward: -0.18\n",
            "Episode 154   Avg Loss: 4.9899, Avg Reward: -2.64\n",
            "Episode 155   Avg Loss: 4.2100, Avg Reward: -1.81\n",
            "Episode 156   Avg Loss: 2.5338, Avg Reward: -2.09\n",
            "Episode 157   Avg Loss: 217.6798, Avg Reward: -0.18\n",
            "Episode 158   Avg Loss: 14.5582, Avg Reward: -1.85\n",
            "Episode 159   Avg Loss: 4.8797, Avg Reward: -2.53\n",
            "Episode 160   Avg Loss: 3.5612, Avg Reward: -2.24\n",
            "Episode 161   Avg Loss: 3.9673, Avg Reward: -2.23\n",
            "Episode 162   Avg Loss: 6.0260, Avg Reward: -1.59\n",
            "Episode 163   Avg Loss: 2.4903, Avg Reward: -2.51\n",
            "Episode 164   Avg Loss: 190.6835, Avg Reward: -2.06\n",
            "Episode 165   Avg Loss: 5.6057, Avg Reward: -1.79\n",
            "Episode 166   Avg Loss: 4.9440, Avg Reward: -2.30\n",
            "Episode 167   Avg Loss: 5.0634, Avg Reward: -1.88\n",
            "Episode 168   Avg Loss: 1.4038, Avg Reward: -1.94\n",
            "Episode 169   Avg Loss: 2.5012, Avg Reward: -2.19\n",
            "Episode 170   Avg Loss: 5.0720, Avg Reward: -2.15\n",
            "Episode 171   Avg Loss: 5.9290, Avg Reward: -2.57\n",
            "Episode 172   Avg Loss: 4.6571, Avg Reward: -2.70\n",
            "Episode 173   Avg Loss: 6.0300, Avg Reward: -2.36\n",
            "Episode 174   Avg Loss: 3.5062, Avg Reward: -2.17\n",
            "Episode 175   Avg Loss: 5.8816, Avg Reward: -1.69\n",
            "Episode 176   Avg Loss: 3.3865, Avg Reward: -1.95\n",
            "Episode 177   Avg Loss: 6.0019, Avg Reward: -1.91\n",
            "Episode 178   Avg Loss: 1.8160, Avg Reward: -2.03\n",
            "Episode 179   Avg Loss: 4.8471, Avg Reward: -1.81\n",
            "Episode 180   Avg Loss: 1.9726, Avg Reward: -2.86\n",
            "Episode 181   Avg Loss: 90.5585, Avg Reward: -2.46\n",
            "Episode 182   Avg Loss: 8.8160, Avg Reward: -1.23\n",
            "Episode 183   Avg Loss: 259.8833, Avg Reward: -3.32\n",
            "Episode 184   Avg Loss: 5.4442, Avg Reward: -1.48\n",
            "Episode 185   Avg Loss: 3.1365, Avg Reward: -2.50\n",
            "Episode 186   Avg Loss: 13.1502, Avg Reward: -1.57\n",
            "Episode 187   Avg Loss: 297.5161, Avg Reward: 0.82\n",
            "Episode 188   Avg Loss: 81.0069, Avg Reward: -1.30\n",
            "Episode 189   Avg Loss: 4.3976, Avg Reward: -2.52\n",
            "Episode 190   Avg Loss: 3.5382, Avg Reward: -1.64\n",
            "Episode 191   Avg Loss: 4.3820, Avg Reward: -1.52\n",
            "Episode 192   Avg Loss: 7.7279, Avg Reward: -2.30\n",
            "Episode 193   Avg Loss: 10.5680, Avg Reward: -2.09\n",
            "Episode 194   Avg Loss: 52.4943, Avg Reward: -1.62\n",
            "Episode 195   Avg Loss: 4.5203, Avg Reward: -1.50\n",
            "Episode 196   Avg Loss: 1.7782, Avg Reward: -2.24\n",
            "Episode 197   Avg Loss: 2.3584, Avg Reward: -1.63\n",
            "Episode 198   Avg Loss: 2.2198, Avg Reward: -2.18\n",
            "Episode 199   Avg Loss: 6.0560, Avg Reward: -1.89\n",
            "Episode 200   Avg Loss: 6.1206, Avg Reward: -1.83\n",
            "Episode 201   Avg Loss: 117.1286, Avg Reward: -1.03\n",
            "Episode 202   Avg Loss: 22.3196, Avg Reward: -1.66\n",
            "Episode 203   Avg Loss: 7.3881, Avg Reward: -2.47\n",
            "Episode 204   Avg Loss: 3.0345, Avg Reward: -1.69\n",
            "Episode 205   Avg Loss: 2.0348, Avg Reward: -2.05\n",
            "Episode 206   Avg Loss: 2.4591, Avg Reward: -2.01\n",
            "Episode 207   Avg Loss: 148.1873, Avg Reward: -0.38\n",
            "Episode 208   Avg Loss: 2.3416, Avg Reward: -2.12\n",
            "Episode 209   Avg Loss: 2.2556, Avg Reward: -1.99\n",
            "Episode 210   Avg Loss: 1.3999, Avg Reward: -2.15\n",
            "Episode 211   Avg Loss: 1.5352, Avg Reward: -2.24\n",
            "Episode 212   Avg Loss: 2.1992, Avg Reward: -2.37\n",
            "Episode 213   Avg Loss: 2.9321, Avg Reward: -2.04\n",
            "Episode 214   Avg Loss: 1.6588, Avg Reward: -2.06\n",
            "Episode 215   Avg Loss: 3.1127, Avg Reward: -2.20\n",
            "Episode 216   Avg Loss: 9.3421, Avg Reward: -2.12\n",
            "Episode 217   Avg Loss: 15.4046, Avg Reward: -1.95\n",
            "Episode 218   Avg Loss: 1.1674, Avg Reward: -1.97\n",
            "Episode 219   Avg Loss: 2.5666, Avg Reward: -2.12\n",
            "Episode 220   Avg Loss: 116.1318, Avg Reward: -0.24\n",
            "Episode 221   Avg Loss: 6.5166, Avg Reward: -1.87\n",
            "Episode 222   Avg Loss: 5.6534, Avg Reward: -1.77\n",
            "Episode 223   Avg Loss: 3.0137, Avg Reward: -1.74\n",
            "Episode 224   Avg Loss: 5.8975, Avg Reward: -2.30\n",
            "Episode 225   Avg Loss: 2.0390, Avg Reward: -2.10\n",
            "Episode 226   Avg Loss: 3.6671, Avg Reward: -2.43\n",
            "Episode 227   Avg Loss: 7.2110, Avg Reward: -1.79\n",
            "Episode 228   Avg Loss: 3.0910, Avg Reward: -1.53\n",
            "Episode 229   Avg Loss: 1.9457, Avg Reward: -1.51\n",
            "Episode 230   Avg Loss: 2.7340, Avg Reward: -2.11\n",
            "Episode 231   Avg Loss: 2.3723, Avg Reward: -1.98\n",
            "Episode 232   Avg Loss: 5.7677, Avg Reward: -1.88\n",
            "Episode 233   Avg Loss: 3.6558, Avg Reward: -1.46\n",
            "Episode 234   Avg Loss: 5.4435, Avg Reward: -1.78\n",
            "Episode 235   Avg Loss: 6.0243, Avg Reward: -2.55\n",
            "Episode 236   Avg Loss: 2.4133, Avg Reward: -2.30\n",
            "Episode 237   Avg Loss: 6.6087, Avg Reward: -1.97\n",
            "Episode 238   Avg Loss: 3.3034, Avg Reward: -2.39\n",
            "Episode 239   Avg Loss: 190.2652, Avg Reward: -1.59\n",
            "Episode 240   Avg Loss: 4.1572, Avg Reward: -2.06\n",
            "Episode 241   Avg Loss: 16.5440, Avg Reward: -2.15\n",
            "Episode 242   Avg Loss: 13.1814, Avg Reward: -1.75\n",
            "Episode 243   Avg Loss: 1.7159, Avg Reward: -2.02\n",
            "Episode 244   Avg Loss: 1.3716, Avg Reward: -1.89\n",
            "Episode 245   Avg Loss: 8.4723, Avg Reward: -1.83\n",
            "Episode 246   Avg Loss: 3.0925, Avg Reward: -2.35\n",
            "Episode 247   Avg Loss: 2.2188, Avg Reward: -2.04\n",
            "Episode 248   Avg Loss: 9.3924, Avg Reward: -2.58\n",
            "Episode 249   Avg Loss: 10.6381, Avg Reward: -1.58\n",
            "Episode 250   Avg Loss: 111.4341, Avg Reward: -0.25\n",
            "Episode 251   Avg Loss: 6.5144, Avg Reward: -1.72\n",
            "Episode 252   Avg Loss: 5.6383, Avg Reward: -1.92\n",
            "Episode 253   Avg Loss: 3.1944, Avg Reward: -1.94\n",
            "Episode 254   Avg Loss: 6.8634, Avg Reward: -1.72\n",
            "Episode 255   Avg Loss: 93.5761, Avg Reward: -0.46\n",
            "Episode 256   Avg Loss: 1.8101, Avg Reward: -2.09\n",
            "Episode 257   Avg Loss: 1.9712, Avg Reward: -2.61\n",
            "Episode 258   Avg Loss: 226.2520, Avg Reward: -3.01\n",
            "Episode 259   Avg Loss: 9.4082, Avg Reward: -1.66\n",
            "Episode 260   Avg Loss: 7.5247, Avg Reward: -1.58\n",
            "Episode 261   Avg Loss: 2.4459, Avg Reward: -2.60\n",
            "Episode 262   Avg Loss: 186.6388, Avg Reward: -0.15\n",
            "Episode 263   Avg Loss: 2.5286, Avg Reward: -2.02\n",
            "Episode 264   Avg Loss: 2.0646, Avg Reward: -2.13\n",
            "Episode 265   Avg Loss: 6.8062, Avg Reward: -1.71\n",
            "Episode 266   Avg Loss: 2.0243, Avg Reward: -1.98\n",
            "Episode 267   Avg Loss: 2.0819, Avg Reward: -1.54\n",
            "Episode 268   Avg Loss: 3.5785, Avg Reward: -2.52\n",
            "Episode 269   Avg Loss: 2.3062, Avg Reward: -2.15\n",
            "Episode 270   Avg Loss: 55.9974, Avg Reward: -1.64\n",
            "Episode 271   Avg Loss: 2.8178, Avg Reward: -1.88\n",
            "Episode 272   Avg Loss: 274.7474, Avg Reward: -2.48\n",
            "Episode 273   Avg Loss: 3963.0003, Avg Reward: -2.86\n",
            "Episode 274   Avg Loss: 1728.3667, Avg Reward: -3.48\n",
            "Episode 275   Avg Loss: 414.1161, Avg Reward: -8.03\n",
            "Episode 276   Avg Loss: 36.9747, Avg Reward: -7.96\n",
            "Episode 277   Avg Loss: 1125.2931, Avg Reward: -5.58\n",
            "Episode 278   Avg Loss: 530.4993, Avg Reward: -2.81\n",
            "Episode 279   Avg Loss: 432.5048, Avg Reward: -2.96\n",
            "Episode 280   Avg Loss: 473.4718, Avg Reward: -3.01\n",
            "Episode 281   Avg Loss: 183.5287, Avg Reward: -3.37\n",
            "Episode 282   Avg Loss: 278.8808, Avg Reward: -3.45\n",
            "Episode 283   Avg Loss: 229.4447, Avg Reward: -1.61\n",
            "Episode 284   Avg Loss: 118.4637, Avg Reward: -4.17\n",
            "Episode 285   Avg Loss: 34.1664, Avg Reward: -3.85\n",
            "Episode 286   Avg Loss: 105.5660, Avg Reward: -1.88\n",
            "Episode 287   Avg Loss: 196.5016, Avg Reward: -0.24\n",
            "Episode 288   Avg Loss: 43.2143, Avg Reward: -1.85\n",
            "Episode 289   Avg Loss: 63.1618, Avg Reward: -1.94\n",
            "Episode 290   Avg Loss: 73.6375, Avg Reward: -3.02\n",
            "Episode 291   Avg Loss: 58.1693, Avg Reward: -2.51\n",
            "Episode 292   Avg Loss: 25.4767, Avg Reward: -3.52\n",
            "Episode 293   Avg Loss: 55.0604, Avg Reward: -1.79\n",
            "Episode 294   Avg Loss: 38.4352, Avg Reward: -1.71\n",
            "Episode 295   Avg Loss: 71.2389, Avg Reward: -1.16\n",
            "Episode 296   Avg Loss: 31.9491, Avg Reward: -1.97\n",
            "Episode 297   Avg Loss: 30.5343, Avg Reward: -1.95\n",
            "Episode 298   Avg Loss: 13.6094, Avg Reward: -3.25\n",
            "Episode 299   Avg Loss: 52.1806, Avg Reward: -1.58\n",
            "0:00:47.968326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jeTuSL9eXAw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved DQL algoirhtm\n",
        "With the baseline DQL algorithm, we don't get good results, the best we can do is having a mean reward close to 0, which is by far from a good performance in this simple game. I am adding 2 things to the above algorithm:\n",
        "\n",
        "### Experience replay\n",
        "One way to improve the performance of the agent is to use what we call an experience replay. This is essentially a memory bank of past experiences that we can collect while running the agent. During the training stage, we can construct our training set by sampling (using simple uniform sampling across all available experiences in the bank).\n",
        "\n",
        "Essentially, in our training procedure, we can now have epochs, each of which consists of 2 parts. First, we play the game with the current state (weights) of the agent and for each step of the simulation, add the experience to the experience replay. An experience consists of a simple training sample, namely the previous state $s$, action $a$, reward $R$ and next state $s'$, esentially a tuple $(a, s, R, s')$. These are enought to feed to the model and get the Q values for the criterion.\n",
        "\n",
        "2 remarks:\n",
        "* First, we sample in order to avoid having the training in sequence and allowing the model to see training data not in sequential order. This is achieved by sampling.\n",
        "* Additionally, it is very important to avoid data drift and hence lower quality predictions. Therefore, it is key to retain older data from the previous epochs and, when adding new data, to remove the oldest ones (in the order they were added to the structure). Essentially, this makes our experience replay a circular buffer that can be implemented using a (FIFO) queue, in a similar fashion to how the queue on MoCo works.\n",
        "\n",
        "Esentially, a queue allows us to add elements to the front (the new data) and eject elements from them back (the old data) in $O(1)$.\n",
        "\n",
        "Unfortunately, the deque implementaion in Python doesn't support access to intermediate elements. Also, it seems that a circular buffer wouldn't work, as it has a fixed size and we need the flexibility to support a varianble amount of experiences (like say in case the last epoch produces more samples than the threshold we set).\n",
        "\n",
        "The best way to do this is to implement a queue using 2 stacks. This is a well know method (see here: https://www.geeksforgeeks.org/queue-using-stacks). A stack can be modeled as a list (which allows for efficiently adding/removing elements at its back). With this setup, we can enable sampling in $O(1)$\n",
        "\n",
        "Let me implement the queue below:"
      ],
      "metadata": {
        "id": "Es7-_1ZSabKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: the code is AI generated for the most part, with my own modifications to make it work properly\n",
        "\n",
        "class Stack:\n",
        "    def __init__(self):\n",
        "        self.stack = []\n",
        "\n",
        "    def get(self, index):\n",
        "        if index >= len(self.stack):\n",
        "            print('Warning: index out-of-bounds, functionality might not work as expected!')\n",
        "\n",
        "        return self.stack[index]\n",
        "\n",
        "    def push(self, element):\n",
        "        self.stack.append(element)\n",
        "\n",
        "    def pop(self):\n",
        "        if not self.is_empty():\n",
        "            return self.stack.pop()\n",
        "\n",
        "    def is_empty(self):\n",
        "        return len(self.stack) == 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stack)\n",
        "\n",
        "class Queue:\n",
        "    def __init__(self):\n",
        "        self.stack1 = Stack()\n",
        "        self.stack2 = Stack()\n",
        "\n",
        "    def get(self, index):\n",
        "        if index >= self.__len__():\n",
        "            print('Warning: index out-of-bounds, functionality might not work as expected!')\n",
        "\n",
        "        if index < len(self.stack1):\n",
        "            return self.stack1.get(index)\n",
        "        else:\n",
        "            return self.stack2.get(index - len(self.stack1))\n",
        "\n",
        "    def push(self, element):\n",
        "        self.stack1.push(element)\n",
        "\n",
        "    def pop(self):\n",
        "        if self.stack2.is_empty():\n",
        "            while not self.stack1.is_empty():\n",
        "                self.stack2.push(self.stack1.pop())\n",
        "\n",
        "        if not self.stack2.is_empty(): # empty means no elements in queue\n",
        "            return self.stack2.pop()\n",
        "\n",
        "    def is_empty(self):\n",
        "        return self.stack1.is_empty() and self.stack2.is_empty()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stack1) + len(self.stack2)\n",
        "\n"
      ],
      "metadata": {
        "id": "l4k5VRtdiCV8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** the order which we do the sampling doesn't matter, as what really matters is to be able to sample every element in the queue unfiformly.\n",
        "\n",
        "Moving on to the experience replay:"
      ],
      "metadata": {
        "id": "Au0kD9Asjwc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "# Structure for a single experience\n",
        "class Experience:\n",
        "    def __init__(self, state, action, reward, next_state, done=False):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.next_state = next_state\n",
        "        self.done = done\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self):\n",
        "        self.memory = Queue()\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.memory.push(experience)\n",
        "\n",
        "    def remove(self):\n",
        "        return self.memory.pop()\n",
        "\n",
        "    def sample(self):\n",
        "        memory_len = len(self.memory)\n",
        "\n",
        "        random_index = np.random.randint(memory_len) # range: [0, memory_len - 1]\n",
        "        return self.memory.get(random_index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "OrR9F8NjdLS4"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some testing\n",
        "\n",
        "exp_replay = ExperienceReplay()\n",
        "exp_replay.add(Experience(1, 1, 1, 1))\n",
        "exp_replay.add(Experience(2, 2, 2, 2))\n",
        "exp_replay.add(Experience(3, 3, 3, 3))\n",
        "\n",
        "print(f'len(exp_replay)={len(exp_replay)}')\n",
        "\n",
        "print([ exp_replay.sample().state for _ in range(10) ]) # values between 1 and 3\n",
        "\n",
        "print(exp_replay.memory.get(0).state) # 1\n",
        "\n",
        "exp_replay.remove()\n",
        "print(exp_replay.memory.get(0).state) # 3\n",
        "print(exp_replay.memory.get(1).state) # 2\n",
        "\n",
        "exp_replay.remove()\n",
        "print(exp_replay.memory.get(0).state) # 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCjt4eQJkw0C",
        "outputId": "37cb63fa-9abe-4316-de8d-a960989bd009"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(exp_replay)=3\n",
            "[1, 1, 2, 3, 3, 1, 3, 1, 2, 1]\n",
            "1\n",
            "3\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Offline/Online networks\n",
        "One more thing we can do is, for each epoch, to keep have an online and an offline network. The offline network (target) estimates the Q value of the current state and the online network estimates the Q value of the next state.\n",
        "\n",
        "Usually, after $C$ steps, we update the parameters of the offline network with the online one and we proceed as usual. In my implementaion, I am doing this once per epoch (every epoch would consist of about 5k training samples).\n",
        "\n",
        "## Implementation details\n",
        "Initially, I fill the experience replay with the 1st epoch experiences. On subsequent epochs, I eject about 1/3rd of the stored experiences and refill it with tne new ones.\n",
        "\n",
        "Regarding training, the new loss function is as follows, given quadruplet $(s, a, r, s')$ and the online $Q_{on}$ and offline $Q_{off}$ networks:\n",
        "$$ \\lVert (r + \\arg\\max_{a'} Q_{off}(s', a')) - Q_{on}(s, a) \\rVert^2 $$\n",
        "\n",
        "For simplicity, I am running a batch gradient descent across the whole dataset at once.\n",
        "\n",
        "To compute the loos we do the following:\n",
        "\n",
        "1. Use the offline network to find the best Q value for the next state, by passing $s'$ to it. ALso, make sure it is frozen or detached, so that it doesn't get updated at all during backpropagation.\n",
        "2. Use the online/current network to compute the Q value for the current state $s$, by taking the action $a$ the agent took during play. Basically, in PyTorch, we need the $a$-th (0-based) index of $Q_{on}(s)$.\n",
        "3. Pass these values to the criterion and apply backprop. Make sure the detach the old model predictions."
      ],
      "metadata": {
        "id": "BaoPq41PmzEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_er(experience_replay, num_experiences_to_remove):\n",
        "    for _ in range(num_experiences_to_remove):\n",
        "        experience_replay.remove()\n",
        "\n",
        "def update_er(experience_replay, new_experiences):\n",
        "    for exp in new_experiences:\n",
        "        experience_replay.add(exp)\n",
        "\n",
        "def play(model, env, epsilon, device, total_new_experiences):\n",
        "    model.eval()\n",
        "    experiences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while len(experiences) < total_new_experiences: # episode\n",
        "            observation, info = env.reset()\n",
        "            observation = torch.tensor(observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "            action = 0\n",
        "\n",
        "            while True:\n",
        "                next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "                next_observation = torch.tensor(next_observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "                done = terminated or truncated # new addition based on the text cell below\n",
        "\n",
        "                #if done:\n",
        "                    #break\n",
        "\n",
        "                # (s, a, r, s')\n",
        "                experience = Experience(observation, action, reward, next_observation, done)\n",
        "                experiences.append(experience)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                q_values = model(next_observation)  # (num_actions)\n",
        "\n",
        "                # update the action\n",
        "\n",
        "                if np.random.rand() < epsilon: # epsilon-greed\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = q_values.argmax().item()  # highest value on the 2nd prediction (next_value)\n",
        "\n",
        "    return experiences\n",
        "\n",
        "def train(offline_model, online_model, optimizer, experience_replay, gamma, device):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    online_model.train()\n",
        "\n",
        "    train_size = 5 * len(experience_replay) # 5 times larger dataset than experiences\n",
        "\n",
        "    # construct the dataset\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(train_size):\n",
        "        dataset.append(experience_replay.sample())\n",
        "\n",
        "    # For simplicity, I will use batch gradient descent (about 5k samples, 8 inputs)\n",
        "    curr_states = torch.stack([ exp.state for exp in dataset ]).to(device) # (train_size, obs_state_dim)\n",
        "    next_states = torch.stack([exp.next_state for exp in dataset]).to(device) # (train_size, obs_state_dim)\n",
        "    actions = torch.tensor([ exp.action for exp in dataset ], dtype=torch.int64, device=device) # (train_size)\n",
        "    rewards = torch.tensor([ exp.reward for exp in dataset ], dtype=torch.float32, device=device) # (train_size)\n",
        "    is_done = torch.tensor([ exp.done for exp in dataset ], dtype=torch.float32, device=device) # (train_size), new addition\n",
        "\n",
        "    # Compute the loss\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Find the best next state Q values\n",
        "    next_state_q_values = offline_model(next_states) # (train_size, num_actions)\n",
        "    best_next_state_q_value = next_state_q_values.max(dim=1).values # (train_size)\n",
        "\n",
        "    # Find the curr state Q value\n",
        "    curr_state_q_values = online_model(curr_states) # (train_size, num_actions)\n",
        "\n",
        "    # The below function finds the index specified by the actions array along dim=1 (so we get the q_value of the corresponding action idx)\n",
        "    curr_state_q_value = curr_state_q_values.gather(1, actions.unsqueeze(1)).squeeze(1) # (train_size)\n",
        "\n",
        "    # Compute the loss & backprop\n",
        "    loss = criterion(curr_state_q_value, rewards + gamma * best_next_state_q_value.detach()) # detach!!\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return statistics\n",
        "    avg_loss = loss.item()\n",
        "    avg_reward = torch.mean(rewards).item()\n",
        "\n",
        "    return avg_loss, avg_reward\n",
        "\n",
        "def create_offline_model(online_model):\n",
        "    offline_model = DQN(online_model.fc1.in_features, online_model.fc3.out_features)\n",
        "    offline_model.load_state_dict(online_model.state_dict())\n",
        "\n",
        "    return offline_model\n",
        "\n",
        "\n",
        "def dql_optimized(env, online_model, optimizer, num_epochs, gamma, epsilon, device, num_experiences):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    experience_replay = ExperienceReplay()\n",
        "\n",
        "    offline_model = create_offline_model(online_model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch > 0:\n",
        "            # Remove 1/3rd of the experinces from the replay\n",
        "            clean_er(experience_replay, num_experiences // 3)\n",
        "\n",
        "        total_new_experiences = num_experiences - len(experience_replay)\n",
        "\n",
        "        # Play phrase\n",
        "        new_experiences = play(online_model, env, epsilon, device, total_new_experiences)\n",
        "        update_er(experience_replay, new_experiences)\n",
        "\n",
        "        # Train phrase\n",
        "        avg_loss, avg_reward = train(offline_model, online_model, optimizer, experience_replay, gamma, device)\n",
        "\n",
        "        # Update offline_model\n",
        "        offline_model.load_state_dict(online_model.state_dict())\n",
        "\n",
        "        print(f'Epoch {epoch+1}   Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.2f}')\n",
        "\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "0bR2Uv-BkyJy"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 40\n",
        "num_experiences = 1000\n",
        "gamma = 0.99\n",
        "epsilon = 0.05\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "model = DQN(obs_state_dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "dql_optimized(env, model, optimizer, num_epochs, gamma, epsilon, device, num_experiences)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "2YY9nkk8lw1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca81748-8c79-4285-97e7-5e2a685ae0f6"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1   Avg Loss: 243.4455, Avg Reward: -8.53\n",
            "Epoch 2   Avg Loss: 232.3337, Avg Reward: -9.27\n",
            "Epoch 3   Avg Loss: 237.4430, Avg Reward: -9.21\n",
            "Epoch 4   Avg Loss: 212.2434, Avg Reward: -8.58\n",
            "Epoch 5   Avg Loss: 258.0019, Avg Reward: -8.48\n",
            "Epoch 6   Avg Loss: 235.0309, Avg Reward: -7.89\n",
            "Epoch 7   Avg Loss: 213.0520, Avg Reward: -7.47\n",
            "Epoch 8   Avg Loss: 192.1595, Avg Reward: -7.01\n",
            "Epoch 9   Avg Loss: 208.6154, Avg Reward: -7.25\n",
            "Epoch 10   Avg Loss: 218.3159, Avg Reward: -7.31\n",
            "Epoch 11   Avg Loss: 184.9039, Avg Reward: -7.16\n",
            "Epoch 12   Avg Loss: 150.4380, Avg Reward: -6.12\n",
            "Epoch 13   Avg Loss: 158.2692, Avg Reward: -5.87\n",
            "Epoch 14   Avg Loss: 95.5065, Avg Reward: -4.80\n",
            "Epoch 15   Avg Loss: 92.2774, Avg Reward: -4.71\n",
            "Epoch 16   Avg Loss: 83.6665, Avg Reward: -4.51\n",
            "Epoch 17   Avg Loss: 114.1888, Avg Reward: -5.04\n",
            "Epoch 18   Avg Loss: 93.5967, Avg Reward: -4.19\n",
            "Epoch 19   Avg Loss: 97.6360, Avg Reward: -4.85\n",
            "Epoch 20   Avg Loss: 69.2206, Avg Reward: -3.52\n",
            "Epoch 21   Avg Loss: 82.5441, Avg Reward: -3.84\n",
            "Epoch 22   Avg Loss: 114.8843, Avg Reward: -2.81\n",
            "Epoch 23   Avg Loss: 120.1279, Avg Reward: -2.51\n",
            "Epoch 24   Avg Loss: 146.8771, Avg Reward: -2.38\n",
            "Epoch 25   Avg Loss: 139.1715, Avg Reward: -2.64\n",
            "Epoch 26   Avg Loss: 159.4588, Avg Reward: -2.73\n",
            "Epoch 27   Avg Loss: 147.6170, Avg Reward: -2.53\n",
            "Epoch 28   Avg Loss: 166.9230, Avg Reward: -2.52\n",
            "Epoch 29   Avg Loss: 169.6969, Avg Reward: -2.30\n",
            "Epoch 30   Avg Loss: 175.0495, Avg Reward: -2.11\n",
            "Epoch 31   Avg Loss: 164.0641, Avg Reward: -1.91\n",
            "Epoch 32   Avg Loss: 185.3676, Avg Reward: -2.14\n",
            "Epoch 33   Avg Loss: 188.0998, Avg Reward: -1.94\n",
            "Epoch 34   Avg Loss: 177.4263, Avg Reward: -2.27\n",
            "Epoch 35   Avg Loss: 177.3362, Avg Reward: -2.17\n",
            "Epoch 36   Avg Loss: 146.8116, Avg Reward: -2.06\n",
            "Epoch 37   Avg Loss: 155.4734, Avg Reward: -2.11\n",
            "Epoch 38   Avg Loss: 154.2865, Avg Reward: -2.27\n",
            "Epoch 39   Avg Loss: 160.3947, Avg Reward: -2.37\n",
            "Epoch 40   Avg Loss: 146.8164, Avg Reward: -2.31\n",
            "0:00:05.132916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** I am still getting bad rewards. After doing some resarch, it seems that we should keep proper track of the final state. Also, include the done flag in the experience replay and only retain the reward $r$ in that case (so no consideration of the next state Q-value).\n",
        "\n",
        "I am adapting the code (so it should be fresh), but the above text cells remain the same."
      ],
      "metadata": {
        "id": "9UxRcJ-Tvuwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Trying some more stuff (like paying with decaying epsilon, a better model, etc.) and also grid search on the hyperparameters:"
      ],
      "metadata": {
        "id": "9M8DCnnR1Fzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dql_optimized_decay_epsilon(env, online_model, optimizer, num_epochs, gamma, epsilon_init, decay_epsilon, device, num_experiences, supress_outputs):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    experience_replay = ExperienceReplay()\n",
        "\n",
        "    offline_model = create_offline_model(online_model)\n",
        "\n",
        "    epsilon = epsilon_init\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch > 0:\n",
        "            # Remove 1/3rd of the experinces from the replay\n",
        "            clean_er(experience_replay, num_experiences // 3)\n",
        "\n",
        "        total_new_experiences = num_experiences - len(experience_replay)\n",
        "\n",
        "        # Play phrase\n",
        "        new_experiences = play(online_model, env, epsilon, device, total_new_experiences)\n",
        "        update_er(experience_replay, new_experiences)\n",
        "\n",
        "        # Train phrase\n",
        "        avg_loss, avg_reward = train(offline_model, online_model, optimizer, experience_replay, gamma, device)\n",
        "\n",
        "        # Update offline_model\n",
        "        offline_model.load_state_dict(online_model.state_dict())\n",
        "\n",
        "        if not supress_outputs or epoch % 10 == 9:\n",
        "            print(f'Epoch {epoch+1}   Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.2f}')\n",
        "\n",
        "        epsilon = max(0.01, epsilon * decay_epsilon)\n",
        "\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "Uk52GUbFlzE_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXI1bXkW1HQY"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "num_experiences = 2000\n",
        "gamma = 0.99\n",
        "epsilon_init = 0.7\n",
        "decay_epsilon = 0.9\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "model = DQN(obs_state_dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "for num_experiences in [ 500, 1000, 2000, 5000 ]:\n",
        "    for epsilon_init in [ .7, .8, .9]:\n",
        "        for decay_epsilon in [ .9, .8, .7 ]:\n",
        "            for lr in [ .0005, .001, .005 ]:\n",
        "                print(f'num_experiences={num_experiences}, epsilon_init={epsilon_init}, decay_epsilon={decay_epsilon}, lr={lr}:')\n",
        "\n",
        "                model = DQN(obs_state_dim, num_actions).to(device)\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                dql_optimized_decay_epsilon(env, model, optimizer, num_epochs, gamma, epsilon_init, decay_epsilon, device, num_experiences, True)\n",
        "\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w6ys5yD5zumw",
        "outputId": "576a9f94-418b-4ece-d399-6da37ee5a7df"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 177.6367, Avg Reward: -3.49\n",
            "Epoch 20   Avg Loss: 172.3517, Avg Reward: -3.20\n",
            "Epoch 30   Avg Loss: 134.8248, Avg Reward: -1.77\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 196.5792, Avg Reward: -6.52\n",
            "Epoch 20   Avg Loss: 274.9839, Avg Reward: -4.45\n",
            "Epoch 30   Avg Loss: 223.9106, Avg Reward: -4.24\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 188.6595, Avg Reward: -2.27\n",
            "Epoch 20   Avg Loss: 174.2901, Avg Reward: -4.32\n",
            "Epoch 30   Avg Loss: 243.0284, Avg Reward: -5.66\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 182.1112, Avg Reward: -4.24\n",
            "Epoch 20   Avg Loss: 176.6966, Avg Reward: -5.66\n",
            "Epoch 30   Avg Loss: 84.7424, Avg Reward: -3.64\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 177.9313, Avg Reward: -3.71\n",
            "Epoch 20   Avg Loss: 189.9365, Avg Reward: -2.86\n",
            "Epoch 30   Avg Loss: 36.9113, Avg Reward: -1.93\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 205.3279, Avg Reward: -7.05\n",
            "Epoch 20   Avg Loss: 258.2881, Avg Reward: -6.29\n",
            "Epoch 30   Avg Loss: 172.0877, Avg Reward: -8.29\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 240.1571, Avg Reward: -7.40\n",
            "Epoch 20   Avg Loss: 171.0319, Avg Reward: -5.30\n",
            "Epoch 30   Avg Loss: 245.2729, Avg Reward: -5.63\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 154.2280, Avg Reward: -7.88\n",
            "Epoch 20   Avg Loss: 256.4426, Avg Reward: -7.62\n",
            "Epoch 30   Avg Loss: 232.9520, Avg Reward: -8.34\n",
            "num_experiences=500, epsilon_init=0.7, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 148.4144, Avg Reward: -6.67\n",
            "Epoch 20   Avg Loss: 244.5560, Avg Reward: -8.16\n",
            "Epoch 30   Avg Loss: 264.0073, Avg Reward: -7.52\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 214.2582, Avg Reward: -5.85\n",
            "Epoch 20   Avg Loss: 167.5849, Avg Reward: -3.97\n",
            "Epoch 30   Avg Loss: 201.3291, Avg Reward: -6.54\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 184.4990, Avg Reward: -5.07\n",
            "Epoch 20   Avg Loss: 178.7932, Avg Reward: -5.03\n",
            "Epoch 30   Avg Loss: 181.6319, Avg Reward: -2.72\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 108.2917, Avg Reward: -4.65\n",
            "Epoch 20   Avg Loss: 318.2862, Avg Reward: -7.31\n",
            "Epoch 30   Avg Loss: 353.4047, Avg Reward: -7.69\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 156.8036, Avg Reward: -2.30\n",
            "Epoch 20   Avg Loss: 182.9953, Avg Reward: -7.02\n",
            "Epoch 30   Avg Loss: 143.4189, Avg Reward: -3.00\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 171.3161, Avg Reward: -3.45\n",
            "Epoch 20   Avg Loss: 244.6041, Avg Reward: -8.25\n",
            "Epoch 30   Avg Loss: 191.9330, Avg Reward: -4.31\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 142.5434, Avg Reward: -7.67\n",
            "Epoch 20   Avg Loss: 287.3842, Avg Reward: -8.02\n",
            "Epoch 30   Avg Loss: 221.4402, Avg Reward: -6.10\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 206.1081, Avg Reward: -4.49\n",
            "Epoch 20   Avg Loss: 150.1214, Avg Reward: -4.14\n",
            "Epoch 30   Avg Loss: 184.9724, Avg Reward: -1.72\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 170.1306, Avg Reward: -7.95\n",
            "Epoch 20   Avg Loss: 99.9389, Avg Reward: -4.62\n",
            "Epoch 30   Avg Loss: 153.3354, Avg Reward: -7.34\n",
            "num_experiences=500, epsilon_init=0.8, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 278.3512, Avg Reward: -7.18\n",
            "Epoch 20   Avg Loss: 365.0538, Avg Reward: -10.19\n",
            "Epoch 30   Avg Loss: 362.0057, Avg Reward: -6.27\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 209.8268, Avg Reward: -2.15\n",
            "Epoch 20   Avg Loss: 151.1009, Avg Reward: -4.06\n",
            "Epoch 30   Avg Loss: 156.4572, Avg Reward: -1.87\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 89.0284, Avg Reward: -4.23\n",
            "Epoch 20   Avg Loss: 203.6975, Avg Reward: -3.46\n",
            "Epoch 30   Avg Loss: 193.2062, Avg Reward: -2.35\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 128.2994, Avg Reward: -4.74\n",
            "Epoch 20   Avg Loss: 145.1064, Avg Reward: -1.74\n",
            "Epoch 30   Avg Loss: 247.5394, Avg Reward: -7.37\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 126.5568, Avg Reward: -5.55\n",
            "Epoch 20   Avg Loss: 120.1836, Avg Reward: -1.20\n",
            "Epoch 30   Avg Loss: 195.2377, Avg Reward: -7.34\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 154.3499, Avg Reward: -3.20\n",
            "Epoch 20   Avg Loss: 168.8651, Avg Reward: -5.71\n",
            "Epoch 30   Avg Loss: 147.1312, Avg Reward: -3.57\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 169.4390, Avg Reward: -6.75\n",
            "Epoch 20   Avg Loss: 243.9835, Avg Reward: -8.64\n",
            "Epoch 30   Avg Loss: 352.2440, Avg Reward: -8.28\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 210.4854, Avg Reward: -6.64\n",
            "Epoch 20   Avg Loss: 184.4157, Avg Reward: -7.74\n",
            "Epoch 30   Avg Loss: 157.2609, Avg Reward: -5.94\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 123.3729, Avg Reward: -2.68\n",
            "Epoch 20   Avg Loss: 187.5726, Avg Reward: -6.51\n",
            "Epoch 30   Avg Loss: 135.3494, Avg Reward: -3.35\n",
            "num_experiences=500, epsilon_init=0.9, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 154.9129, Avg Reward: -2.10\n",
            "Epoch 20   Avg Loss: 169.6702, Avg Reward: -4.81\n",
            "Epoch 30   Avg Loss: 308.6681, Avg Reward: -9.26\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 182.6626, Avg Reward: -6.92\n",
            "Epoch 20   Avg Loss: 225.9762, Avg Reward: -8.07\n",
            "Epoch 30   Avg Loss: 191.8015, Avg Reward: -5.72\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 103.2271, Avg Reward: -4.42\n",
            "Epoch 20   Avg Loss: 169.2917, Avg Reward: -2.40\n",
            "Epoch 30   Avg Loss: 200.2478, Avg Reward: -8.71\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 195.3978, Avg Reward: -6.57\n",
            "Epoch 20   Avg Loss: 220.7572, Avg Reward: -4.57\n",
            "Epoch 30   Avg Loss: 522.3782, Avg Reward: -6.28\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 186.0166, Avg Reward: -6.34\n",
            "Epoch 20   Avg Loss: 216.5983, Avg Reward: -7.76\n",
            "Epoch 30   Avg Loss: 149.3007, Avg Reward: -7.71\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 101.6116, Avg Reward: -1.42\n",
            "Epoch 20   Avg Loss: 52.0279, Avg Reward: -0.80\n",
            "Epoch 30   Avg Loss: 93.6158, Avg Reward: -1.87\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 131.3853, Avg Reward: -1.93\n",
            "Epoch 20   Avg Loss: 181.2603, Avg Reward: -6.24\n",
            "Epoch 30   Avg Loss: 211.4226, Avg Reward: -6.43\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 250.8087, Avg Reward: -8.78\n",
            "Epoch 20   Avg Loss: 243.8340, Avg Reward: -8.52\n",
            "Epoch 30   Avg Loss: 186.6038, Avg Reward: -7.03\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 142.4816, Avg Reward: -6.46\n",
            "Epoch 20   Avg Loss: 305.8756, Avg Reward: -7.74\n",
            "Epoch 30   Avg Loss: 126.4936, Avg Reward: -6.51\n",
            "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 220.4641, Avg Reward: -6.44\n",
            "Epoch 20   Avg Loss: 192.8143, Avg Reward: -1.93\n",
            "Epoch 30   Avg Loss: 396.1192, Avg Reward: -7.23\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 190.7478, Avg Reward: -5.54\n",
            "Epoch 20   Avg Loss: 196.2434, Avg Reward: -6.23\n",
            "Epoch 30   Avg Loss: 222.2022, Avg Reward: -7.94\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 152.0460, Avg Reward: -5.50\n",
            "Epoch 20   Avg Loss: 159.0894, Avg Reward: -1.99\n",
            "Epoch 30   Avg Loss: 66.2755, Avg Reward: -2.09\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 209.9548, Avg Reward: -6.81\n",
            "Epoch 20   Avg Loss: 170.7874, Avg Reward: -5.97\n",
            "Epoch 30   Avg Loss: 569.0363, Avg Reward: -6.89\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 183.7272, Avg Reward: -5.64\n",
            "Epoch 20   Avg Loss: 188.2529, Avg Reward: -5.80\n",
            "Epoch 30   Avg Loss: 178.5316, Avg Reward: -7.11\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 216.3076, Avg Reward: -7.82\n",
            "Epoch 20   Avg Loss: 217.9979, Avg Reward: -8.30\n",
            "Epoch 30   Avg Loss: 237.2365, Avg Reward: -7.94\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 155.0148, Avg Reward: -1.78\n",
            "Epoch 20   Avg Loss: 219.7930, Avg Reward: -7.47\n",
            "Epoch 30   Avg Loss: 142.8400, Avg Reward: -2.02\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 216.9627, Avg Reward: -8.59\n",
            "Epoch 20   Avg Loss: 187.1903, Avg Reward: -6.73\n",
            "Epoch 30   Avg Loss: 207.7514, Avg Reward: -5.65\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 138.9120, Avg Reward: -2.50\n",
            "Epoch 20   Avg Loss: 125.4892, Avg Reward: -2.26\n",
            "Epoch 30   Avg Loss: 91.3314, Avg Reward: -2.87\n",
            "num_experiences=1000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 163.8676, Avg Reward: -3.33\n",
            "Epoch 20   Avg Loss: 362.8409, Avg Reward: -9.14\n",
            "Epoch 30   Avg Loss: 179.6438, Avg Reward: -2.16\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 143.5523, Avg Reward: -3.50\n",
            "Epoch 20   Avg Loss: 122.8430, Avg Reward: -2.15\n",
            "Epoch 30   Avg Loss: 176.4649, Avg Reward: -2.94\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 206.7364, Avg Reward: -3.20\n",
            "Epoch 20   Avg Loss: 116.5042, Avg Reward: -3.19\n",
            "Epoch 30   Avg Loss: 161.4038, Avg Reward: -2.88\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 143.5418, Avg Reward: -6.04\n",
            "Epoch 20   Avg Loss: 289.6720, Avg Reward: -7.69\n",
            "Epoch 30   Avg Loss: 453.4794, Avg Reward: -6.93\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 223.7256, Avg Reward: -7.41\n",
            "Epoch 20   Avg Loss: 197.7290, Avg Reward: -6.84\n",
            "Epoch 30   Avg Loss: 182.5551, Avg Reward: -7.35\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 233.4241, Avg Reward: -7.85\n",
            "Epoch 20   Avg Loss: 82.8038, Avg Reward: -2.17\n",
            "Epoch 30   Avg Loss: 148.2449, Avg Reward: -3.90\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 134.9723, Avg Reward: -5.14\n",
            "Epoch 20   Avg Loss: 279.7682, Avg Reward: -8.67\n",
            "Epoch 30   Avg Loss: 155.4372, Avg Reward: -7.04\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 128.8167, Avg Reward: -2.96\n",
            "Epoch 20   Avg Loss: 157.9261, Avg Reward: -1.74\n",
            "Epoch 30   Avg Loss: 175.1981, Avg Reward: -3.93\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 211.6001, Avg Reward: -1.94\n",
            "Epoch 20   Avg Loss: 188.8125, Avg Reward: -2.69\n",
            "Epoch 30   Avg Loss: 154.9697, Avg Reward: -1.61\n",
            "num_experiences=1000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 189.4618, Avg Reward: -1.65\n",
            "Epoch 20   Avg Loss: 237.0791, Avg Reward: -7.53\n",
            "Epoch 30   Avg Loss: 201.2942, Avg Reward: -8.37\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 89.4251, Avg Reward: -3.41\n",
            "Epoch 20   Avg Loss: 96.2932, Avg Reward: -3.48\n",
            "Epoch 30   Avg Loss: 84.7949, Avg Reward: -2.77\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 162.9841, Avg Reward: -3.78\n",
            "Epoch 20   Avg Loss: 210.0250, Avg Reward: -7.24\n",
            "Epoch 30   Avg Loss: 191.6936, Avg Reward: -5.92\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 126.9675, Avg Reward: -2.82\n",
            "Epoch 20   Avg Loss: 183.0313, Avg Reward: -5.45\n",
            "Epoch 30   Avg Loss: 136.3284, Avg Reward: -1.84\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 157.9932, Avg Reward: -4.72\n",
            "Epoch 20   Avg Loss: 154.0113, Avg Reward: -3.31\n",
            "Epoch 30   Avg Loss: 54.7219, Avg Reward: -1.41\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 90.8491, Avg Reward: -2.16\n",
            "Epoch 20   Avg Loss: 164.0198, Avg Reward: -4.85\n",
            "Epoch 30   Avg Loss: 198.4858, Avg Reward: -7.39\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 165.2482, Avg Reward: -5.88\n",
            "Epoch 20   Avg Loss: 330.1922, Avg Reward: -6.56\n",
            "Epoch 30   Avg Loss: 256.7953, Avg Reward: -7.76\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 101.0632, Avg Reward: -1.64\n",
            "Epoch 20   Avg Loss: 141.5659, Avg Reward: -2.62\n",
            "Epoch 30   Avg Loss: 121.9993, Avg Reward: -0.92\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 163.0594, Avg Reward: -3.46\n",
            "Epoch 20   Avg Loss: 71.7493, Avg Reward: -4.01\n",
            "Epoch 30   Avg Loss: 202.1352, Avg Reward: -5.54\n",
            "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 251.8671, Avg Reward: -8.11\n",
            "Epoch 20   Avg Loss: 246.9945, Avg Reward: -8.06\n",
            "Epoch 30   Avg Loss: 216.3134, Avg Reward: -1.95\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 83.9898, Avg Reward: -2.41\n",
            "Epoch 20   Avg Loss: 93.1486, Avg Reward: -1.38\n",
            "Epoch 30   Avg Loss: 153.6644, Avg Reward: -5.88\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 103.3109, Avg Reward: -1.74\n",
            "Epoch 20   Avg Loss: 148.9718, Avg Reward: -3.41\n",
            "Epoch 30   Avg Loss: 180.3835, Avg Reward: -3.79\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 236.8491, Avg Reward: -6.24\n",
            "Epoch 20   Avg Loss: 209.6068, Avg Reward: -7.35\n",
            "Epoch 30   Avg Loss: 180.6634, Avg Reward: -8.37\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 171.0021, Avg Reward: -6.75\n",
            "Epoch 20   Avg Loss: 202.5725, Avg Reward: -9.12\n",
            "Epoch 30   Avg Loss: 146.8016, Avg Reward: -6.57\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 164.2934, Avg Reward: -2.69\n",
            "Epoch 20   Avg Loss: 228.4678, Avg Reward: -5.02\n",
            "Epoch 30   Avg Loss: 117.8301, Avg Reward: -4.42\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 247.6751, Avg Reward: -7.95\n",
            "Epoch 20   Avg Loss: 274.5451, Avg Reward: -8.26\n",
            "Epoch 30   Avg Loss: 201.2995, Avg Reward: -6.25\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 177.1984, Avg Reward: -1.78\n",
            "Epoch 20   Avg Loss: 153.6817, Avg Reward: -6.19\n",
            "Epoch 30   Avg Loss: 184.3700, Avg Reward: -5.76\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 218.6092, Avg Reward: -6.76\n",
            "Epoch 20   Avg Loss: 203.9204, Avg Reward: -7.18\n",
            "Epoch 30   Avg Loss: 223.6553, Avg Reward: -6.61\n",
            "num_experiences=2000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 286.3764, Avg Reward: -8.06\n",
            "Epoch 20   Avg Loss: 548.1143, Avg Reward: -7.56\n",
            "Epoch 30   Avg Loss: 195.6270, Avg Reward: -4.37\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 178.8052, Avg Reward: -2.71\n",
            "Epoch 20   Avg Loss: 152.1677, Avg Reward: -1.46\n",
            "Epoch 30   Avg Loss: 165.0005, Avg Reward: -1.84\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 145.0710, Avg Reward: -1.88\n",
            "Epoch 20   Avg Loss: 134.2049, Avg Reward: -2.76\n",
            "Epoch 30   Avg Loss: 172.5082, Avg Reward: -2.81\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 154.8046, Avg Reward: -5.13\n",
            "Epoch 20   Avg Loss: 189.9800, Avg Reward: -5.75\n",
            "Epoch 30   Avg Loss: 186.7681, Avg Reward: -7.32\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 125.5978, Avg Reward: -5.12\n",
            "Epoch 20   Avg Loss: 252.1380, Avg Reward: -8.11\n",
            "Epoch 30   Avg Loss: 88.9056, Avg Reward: -2.18\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 87.0783, Avg Reward: -3.00\n",
            "Epoch 20   Avg Loss: 146.3064, Avg Reward: -1.84\n",
            "Epoch 30   Avg Loss: 179.7101, Avg Reward: -6.54\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 70.4487, Avg Reward: -0.99\n",
            "Epoch 20   Avg Loss: 162.0199, Avg Reward: -5.73\n",
            "Epoch 30   Avg Loss: 243.7314, Avg Reward: -7.00\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 108.5418, Avg Reward: -5.39\n",
            "Epoch 20   Avg Loss: 116.3646, Avg Reward: -4.29\n",
            "Epoch 30   Avg Loss: 162.6796, Avg Reward: -5.10\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 116.9060, Avg Reward: -2.80\n",
            "Epoch 20   Avg Loss: 73.1622, Avg Reward: -1.36\n",
            "Epoch 30   Avg Loss: 116.0432, Avg Reward: -3.41\n",
            "num_experiences=2000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 161.6002, Avg Reward: -2.33\n",
            "Epoch 20   Avg Loss: 272.9183, Avg Reward: -8.18\n",
            "Epoch 30   Avg Loss: 177.0857, Avg Reward: -1.68\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 159.7872, Avg Reward: -1.90\n",
            "Epoch 20   Avg Loss: 201.3287, Avg Reward: -6.09\n",
            "Epoch 30   Avg Loss: 168.4126, Avg Reward: -3.38\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 159.8811, Avg Reward: -5.49\n",
            "Epoch 20   Avg Loss: 159.5867, Avg Reward: -3.62\n",
            "Epoch 30   Avg Loss: 115.2780, Avg Reward: -2.10\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 184.3755, Avg Reward: -5.05\n",
            "Epoch 20   Avg Loss: 276.7701, Avg Reward: -8.31\n",
            "Epoch 30   Avg Loss: 194.9635, Avg Reward: -2.38\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 146.2609, Avg Reward: -1.79\n",
            "Epoch 20   Avg Loss: 171.5316, Avg Reward: -6.22\n",
            "Epoch 30   Avg Loss: 188.0679, Avg Reward: -5.53\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 156.1218, Avg Reward: -2.51\n",
            "Epoch 20   Avg Loss: 132.8103, Avg Reward: -4.44\n",
            "Epoch 30   Avg Loss: 216.2823, Avg Reward: -7.24\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 204.8430, Avg Reward: -7.01\n",
            "Epoch 20   Avg Loss: 437.1404, Avg Reward: -7.04\n",
            "Epoch 30   Avg Loss: 312.5091, Avg Reward: -8.17\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 206.4227, Avg Reward: -5.14\n",
            "Epoch 20   Avg Loss: 208.2913, Avg Reward: -5.49\n",
            "Epoch 30   Avg Loss: 189.9428, Avg Reward: -6.32\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 155.1463, Avg Reward: -7.05\n",
            "Epoch 20   Avg Loss: 176.1214, Avg Reward: -5.39\n",
            "Epoch 30   Avg Loss: 196.0060, Avg Reward: -5.29\n",
            "num_experiences=5000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 155.5787, Avg Reward: -6.40\n",
            "Epoch 20   Avg Loss: 244.6231, Avg Reward: -7.85\n",
            "Epoch 30   Avg Loss: 178.0949, Avg Reward: -2.02\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 170.3408, Avg Reward: -5.03\n",
            "Epoch 20   Avg Loss: 161.4134, Avg Reward: -2.57\n",
            "Epoch 30   Avg Loss: 181.5934, Avg Reward: -4.58\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 132.2563, Avg Reward: -1.52\n",
            "Epoch 20   Avg Loss: 82.8670, Avg Reward: -3.02\n",
            "Epoch 30   Avg Loss: 115.5278, Avg Reward: -1.26\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 118.9077, Avg Reward: -4.04\n",
            "Epoch 20   Avg Loss: 205.7980, Avg Reward: -5.68\n",
            "Epoch 30   Avg Loss: 228.1155, Avg Reward: -8.21\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 87.1961, Avg Reward: -2.45\n",
            "Epoch 20   Avg Loss: 164.7352, Avg Reward: -3.33\n",
            "Epoch 30   Avg Loss: 166.4055, Avg Reward: -5.58\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 145.5968, Avg Reward: -1.76\n",
            "Epoch 20   Avg Loss: 155.4640, Avg Reward: -1.38\n",
            "Epoch 30   Avg Loss: 140.5167, Avg Reward: -6.16\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 152.4124, Avg Reward: -1.83\n",
            "Epoch 20   Avg Loss: 267.6961, Avg Reward: -8.21\n",
            "Epoch 30   Avg Loss: 163.2100, Avg Reward: -1.87\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 127.0971, Avg Reward: -6.33\n",
            "Epoch 20   Avg Loss: 229.9340, Avg Reward: -7.37\n",
            "Epoch 30   Avg Loss: 193.4671, Avg Reward: -6.19\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 219.5677, Avg Reward: -7.74\n",
            "Epoch 20   Avg Loss: 114.9011, Avg Reward: -1.23\n",
            "Epoch 30   Avg Loss: 206.5023, Avg Reward: -7.44\n",
            "num_experiences=5000, epsilon_init=0.8, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 159.5429, Avg Reward: -2.22\n",
            "Epoch 20   Avg Loss: 157.3622, Avg Reward: -4.82\n",
            "Epoch 30   Avg Loss: 162.4581, Avg Reward: -6.29\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 152.3114, Avg Reward: -1.93\n",
            "Epoch 20   Avg Loss: 140.9410, Avg Reward: -2.54\n",
            "Epoch 30   Avg Loss: 147.5690, Avg Reward: -1.99\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.001:\n",
            "Epoch 10   Avg Loss: 158.7943, Avg Reward: -2.75\n",
            "Epoch 20   Avg Loss: 167.7460, Avg Reward: -3.56\n",
            "Epoch 30   Avg Loss: 151.5312, Avg Reward: -2.55\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.9, lr=0.005:\n",
            "Epoch 10   Avg Loss: 123.7635, Avg Reward: -4.16\n",
            "Epoch 20   Avg Loss: 298.5877, Avg Reward: -5.35\n",
            "Epoch 30   Avg Loss: 1805.5140, Avg Reward: -6.15\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 167.4063, Avg Reward: -5.73\n",
            "Epoch 20   Avg Loss: 205.4069, Avg Reward: -2.25\n",
            "Epoch 30   Avg Loss: 157.9330, Avg Reward: -1.41\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.001:\n",
            "Epoch 10   Avg Loss: 160.3778, Avg Reward: -6.27\n",
            "Epoch 20   Avg Loss: 177.4428, Avg Reward: -7.08\n",
            "Epoch 30   Avg Loss: 187.3360, Avg Reward: -6.87\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.8, lr=0.005:\n",
            "Epoch 10   Avg Loss: 190.8021, Avg Reward: -6.91\n",
            "Epoch 20   Avg Loss: 300.8095, Avg Reward: -8.34\n",
            "Epoch 30   Avg Loss: 153.5565, Avg Reward: -4.33\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.0005:\n",
            "Epoch 10   Avg Loss: 172.7551, Avg Reward: -7.49\n",
            "Epoch 20   Avg Loss: 162.4735, Avg Reward: -6.09\n",
            "Epoch 30   Avg Loss: 179.7624, Avg Reward: -3.10\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.001:\n",
            "Epoch 10   Avg Loss: 144.1331, Avg Reward: -4.88\n",
            "Epoch 20   Avg Loss: 95.7789, Avg Reward: -1.15\n",
            "Epoch 30   Avg Loss: 163.4108, Avg Reward: -5.82\n",
            "num_experiences=5000, epsilon_init=0.9, decay_epsilon=0.7, lr=0.005:\n",
            "Epoch 10   Avg Loss: 185.5554, Avg Reward: -4.13\n",
            "Epoch 20   Avg Loss: 195.1674, Avg Reward: -7.24\n",
            "Epoch 30   Avg Loss: 195.1302, Avg Reward: -6.34\n",
            "0:15:50.249516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nstart = datetime.now()\\n\\ndql_optimized_decay_espsilon(env, model, optimizer, num_epochs, gamma, epsilon_init, decay_epsilon, device, num_experiences)\\n\\nend = datetime.now()\\n\\nprint(end - start)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Runs:\n",
        "\n",
        "'''\n",
        "num_experiences=2000, epsilon_init=0.7, decay_epsilon=0.7, lr=0.0005:\n",
        "Epoch 10   Avg Loss: 101.0632, Avg Reward: -1.64\n",
        "Epoch 20   Avg Loss: 141.5659, Avg Reward: -2.62\n",
        "Epoch 30   Avg Loss: 121.9993, Avg Reward: -0.92\n",
        "'''\n",
        "\n",
        "'''\n",
        "num_experiences=1000, epsilon_init=0.7, decay_epsilon=0.8, lr=0.001:\n",
        "Epoch 10   Avg Loss: 101.6116, Avg Reward: -1.42\n",
        "Epoch 20   Avg Loss: 52.0279, Avg Reward: -0.80\n",
        "Epoch 30   Avg Loss: 93.6158, Avg Reward: -1.87\n",
        "'''"
      ],
      "metadata": {
        "id": "n_ZjATIE0E0N"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double DQL\n",
        "Moving on to the double Deep Q-Learning approach. It is similar to the offline/online approach, with the key difference being that the online network select the next action to take (ie. finds the action $a'$ that yields the best Q-value on the next state $s'$) and the offline network is used to evaluate the next state based solely on this selected action. And we also use the online network to find the Q-value of the current state $s$.\n",
        "\n",
        "Here is the loss function for double DQL:\n",
        "$$ \\lVert (r + Q_{off}(s', a')) - Q_{on}(s, a) \\rVert^2 $$\n",
        "\n",
        "where $$ a' = \\arg\\max_{a'} Q_{on}(s', a') $$\n",
        "\n",
        "For comparison this is the loss function for the online/offline approach:\n",
        "$$ \\lVert (r + \\arg\\max_{a'} Q_{off}(s', a')) - Q_{on}(s, a) \\rVert^2 $$\n",
        "\n",
        "Notice that the only differece between the 2 is that we use the online network to select the best action for the next state instead of using the offline for both this and evaluation like the previous approach."
      ],
      "metadata": {
        "id": "1u-kDETj9HTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dql(offline_model, online_model, optimizer, experience_replay, gamma, device):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    online_model.train()\n",
        "\n",
        "    train_size = 5 * len(experience_replay) # 5 times larger dataset than experiences\n",
        "\n",
        "    # construct the dataset\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(train_size):\n",
        "        dataset.append(experience_replay.sample())\n",
        "\n",
        "    # For simplicity, I will use batch gradient descent (about 5k samples, 8 inputs)\n",
        "    curr_states = torch.stack([ exp.state for exp in dataset ]).to(device) # (train_size, obs_state_dim)\n",
        "    next_states = torch.stack([exp.next_state for exp in dataset]).to(device) # (train_size, obs_state_dim)\n",
        "    actions = torch.tensor([ exp.action for exp in dataset ], dtype=torch.int64, device=device) # (train_size)\n",
        "    rewards = torch.tensor([ exp.reward for exp in dataset ], dtype=torch.float32, device=device) # (train_size)\n",
        "    is_done = torch.tensor([ exp.done for exp in dataset ], dtype=torch.float32, device=device) # (train_size), new addition\n",
        "\n",
        "    # Compute the loss\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ### START Different with the regural offline/online training ###\n",
        "\n",
        "    # Find the best next state Q values\n",
        "    next_state_q_values = online_model(next_states) # (train_size, num_actions)\n",
        "\n",
        "    a_prime = next_state_q_values.argmax(dim=1) # (train_size), the next state selected actions\n",
        "\n",
        "    # (same trick as below to get the right indices)\n",
        "    next_action_q_value = offline_model(next_states).gather(1, a_prime.unsqueeze(1)).squeeze(1) # (train_size)\n",
        "\n",
        "    ### END Different with the regural offline/online training ###\n",
        "\n",
        "    # Find the curr state Q value\n",
        "    curr_state_q_values = online_model(curr_states) # (train_size, num_actions)\n",
        "\n",
        "    # The below function finds the index specified by the actions array along dim=1 (so we get the q_value of the corresponding action idx)\n",
        "    curr_state_q_value = curr_state_q_values.gather(1, actions.unsqueeze(1)).squeeze(1) # (train_size)\n",
        "\n",
        "    # Compute the loss & backprop\n",
        "    loss = criterion(curr_state_q_value, rewards + gamma * next_action_q_value.detach()) # detach!!\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return statistics\n",
        "    avg_loss = loss.item()\n",
        "    avg_reward = torch.mean(rewards).item()\n",
        "\n",
        "    return avg_loss, avg_reward\n",
        "\n",
        "def double_dql(env, online_model, optimizer, num_epochs, gamma, epsilon, device, num_experiences):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    experience_replay = ExperienceReplay()\n",
        "\n",
        "    offline_model = create_offline_model(online_model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch > 0:\n",
        "            # Remove 1/3rd of the experinces from the replay\n",
        "            clean_er(experience_replay, num_experiences // 3)\n",
        "\n",
        "        total_new_experiences = num_experiences - len(experience_replay)\n",
        "\n",
        "        # Play phrase\n",
        "        new_experiences = play(online_model, env, epsilon, device, total_new_experiences)\n",
        "        update_er(experience_replay, new_experiences)\n",
        "\n",
        "        # Train phrase\n",
        "        # !! the only difference in this line !!\n",
        "        avg_loss, avg_reward = train_dql(offline_model, online_model, optimizer, experience_replay, gamma, device)\n",
        "\n",
        "        # Update offline_model\n",
        "        offline_model.load_state_dict(online_model.state_dict())\n",
        "\n",
        "        print(f'Epoch {epoch+1}   Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.2f}')\n",
        "\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "C8gMXiyS8vQU"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 40\n",
        "num_experiences = 1000\n",
        "gamma = 0.99\n",
        "epsilon = 0.05\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "model = DQN(obs_state_dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "double_dql(env, model, optimizer, num_epochs, gamma, epsilon, device, num_experiences)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzzUI-Nc8tNb",
        "outputId": "893f3de4-f718-4afc-d0f0-46e73865bac5"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1   Avg Loss: 162.2543, Avg Reward: -2.02\n",
            "Epoch 2   Avg Loss: 173.9440, Avg Reward: -1.86\n",
            "Epoch 3   Avg Loss: 170.6209, Avg Reward: -2.61\n",
            "Epoch 4   Avg Loss: 157.2971, Avg Reward: -3.08\n",
            "Epoch 5   Avg Loss: 157.1756, Avg Reward: -4.89\n",
            "Epoch 6   Avg Loss: 123.8236, Avg Reward: -4.82\n",
            "Epoch 7   Avg Loss: 203.3733, Avg Reward: -6.47\n",
            "Epoch 8   Avg Loss: 244.4152, Avg Reward: -6.80\n",
            "Epoch 9   Avg Loss: 253.5333, Avg Reward: -7.57\n",
            "Epoch 10   Avg Loss: 228.4135, Avg Reward: -7.58\n",
            "Epoch 11   Avg Loss: 229.0850, Avg Reward: -6.93\n",
            "Epoch 12   Avg Loss: 173.1917, Avg Reward: -4.98\n",
            "Epoch 13   Avg Loss: 145.6154, Avg Reward: -3.18\n",
            "Epoch 14   Avg Loss: 139.6315, Avg Reward: -3.17\n",
            "Epoch 15   Avg Loss: 212.2620, Avg Reward: -5.27\n",
            "Epoch 16   Avg Loss: 209.5178, Avg Reward: -6.80\n",
            "Epoch 17   Avg Loss: 245.0399, Avg Reward: -7.21\n",
            "Epoch 18   Avg Loss: 233.7222, Avg Reward: -6.81\n",
            "Epoch 19   Avg Loss: 225.4056, Avg Reward: -6.95\n",
            "Epoch 20   Avg Loss: 197.8004, Avg Reward: -5.70\n",
            "Epoch 21   Avg Loss: 187.9504, Avg Reward: -4.88\n",
            "Epoch 22   Avg Loss: 174.2718, Avg Reward: -3.11\n",
            "Epoch 23   Avg Loss: 166.5759, Avg Reward: -2.44\n",
            "Epoch 24   Avg Loss: 147.7362, Avg Reward: -2.11\n",
            "Epoch 25   Avg Loss: 145.3373, Avg Reward: -1.87\n",
            "Epoch 26   Avg Loss: 147.2332, Avg Reward: -2.31\n",
            "Epoch 27   Avg Loss: 127.9450, Avg Reward: -2.48\n",
            "Epoch 28   Avg Loss: 161.7906, Avg Reward: -3.82\n",
            "Epoch 29   Avg Loss: 155.4230, Avg Reward: -5.10\n",
            "Epoch 30   Avg Loss: 160.8310, Avg Reward: -6.18\n",
            "Epoch 31   Avg Loss: 164.7767, Avg Reward: -6.81\n",
            "Epoch 32   Avg Loss: 153.0389, Avg Reward: -6.43\n",
            "Epoch 33   Avg Loss: 133.7533, Avg Reward: -6.14\n",
            "Epoch 34   Avg Loss: 137.6395, Avg Reward: -6.13\n",
            "Epoch 35   Avg Loss: 171.6329, Avg Reward: -6.36\n",
            "Epoch 36   Avg Loss: 135.2707, Avg Reward: -5.28\n",
            "Epoch 37   Avg Loss: 204.1787, Avg Reward: -6.34\n",
            "Epoch 38   Avg Loss: 212.9380, Avg Reward: -6.93\n",
            "Epoch 39   Avg Loss: 222.1869, Avg Reward: -8.20\n",
            "Epoch 40   Avg Loss: 187.1475, Avg Reward: -7.82\n",
            "0:00:06.768648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEmriUyjCgN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy-based Deep RL\n",
        "On the 2nd part of the notebook, I am diving into some policy-based RL algoithms.\n",
        "\n",
        "The idea behind policy-based RL algorithms is to have an agent (in the form of a deep neural network) making decisions on the policy function. Concretely, it parameterizes the stochastic policy function $\\pi_{θ}$, which outputs a probabilistic distribution on the actions space given a state $s$:\n",
        "\n",
        "$$ \\pi_{\\theta}(a_0 \\vert s_0) = pr(a = a_0 \\vert s = s_0) $$\n",
        "\n",
        "### Policy gradient\n",
        "The fundmental policy-based method is policy gradient. The objective of policy gradient is to maximize the expected reward if we start from an initial state $s_0$:\n",
        "\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau}[R] = \\mathbb{E}_{\\tau}[r_0 + \\gamma r_1 + \\gamma^2r_2 + \\cdots \\vert s_0, \\pi_{\\theta}] $$\n",
        "\n",
        "Essentially, what this means is that we need to find such a policy $\\pi_{\\theta}$, such that, among all possible trajectories (paths) $\\tau = (s_0, a_0, r_0, s_1, \\cdots)$ that can possible occur with any probability, the expected reward we get is maximized. Essentially, we want to maximize our expected reward starting from the initial state $s_0$.\n",
        "\n",
        "Policy gradient is about optimizing this objective. This can be done by gradient ascent, that is we do gradient descent but we move on the opposite side, in order to maximize our objective (instead of say minimizing a loss function). However, with an easy trick (trying to minimize $-J(\\theta)$), we can use our familar gradient descent.\n",
        "\n",
        "Okay, now let's try to compute the gradient:\n",
        "$$ \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\mathbb{E}_{\\tau}[R] $$.\n",
        "\n",
        "Having the gradient outside of the expectation doesn't help much. We should try to get it inside, as it will be very helpful with many practical policy-based algorithms.\n",
        "\n",
        "One way to do this is using the log trick. We have\n",
        "$$\n",
        "\\nabla_{\\theta} lnf(\\theta) = \\frac {\\nabla_{\\theta} f(\\theta)} {f(\\theta)} ⟺ \\nabla_{\\theta} f(\\theta) = f(\\theta) \\nabla_{\\theta} lnf(\\theta)\n",
        "\\tag{1}\n",
        "$$\n",
        "\n",
        "We can expand our expectation (integration or summation, depending if we have a continuous or discrete state/action space):\n",
        "$$ \\nabla_{\\theta} \\mathbb{E}_{\\tau}[R] =\n",
        "\\nabla_{\\theta} \\sum_{\\tau' \\in \\tau} p(\\tau' | s_0, \\pi(\\theta)) R =\n",
        "\\sum_{\\tau' \\in \\tau} (\\nabla_{\\theta} p(\\tau')) \\cdot R \\overset{(1)}{=}\n",
        "\\sum_{\\tau' \\in \\tau} p(\\tau') \\cdot (\\nabla_{\\theta} ln p(\\tau') \\cdot R ) = \\mathbb{E}_{\\tau}[\\nabla_{\\theta} lnp(\\tau') \\cdot R]\n",
        "\\tag{2}\n",
        "$$\n",
        "\n",
        "Note that this works both for integrals and summations and we can treat R as a constant in terms or $\\theta$, so can factor it out of the gradient.\n",
        "\n",
        "Okay, now let's try to compute the probability of a trajectory $\\tau'$ to occur given our policy and the initial state. $\\tau'$ can be decomposed to a path of the form $\\tau' = (s_0, a_0, r_0, s_1, a_1, r_1, \\cdots)$, essentially a sequence of a feeback loop of a state fed to the agent, the action taken by the agent, the reward given the action and the state and transition to a new state. One thing to note here is that we assume that the game respects the markovian property, that is both the enviornment and the agent operate based on the current state $s_{t}$ and do not consider the prior ones.\n",
        "\n",
        "This allows us to compute the probability easily:\n",
        "$$ p(\\tau') = \\pi_\\theta (a_0 \\vert s_0) p(s_1 | a_0, s_0) \\pi_\\theta (a_1 \\vert s_1) p(s_2 | a_1, s_1) \\cdots = \\prod_{i=0} \\pi_\\theta (a_i \\vert s_i) p(s_{i+1} | a_i, s_i)  $$\n",
        "\n",
        "The neat thing is that, with the logarithm we introduced earlier, we can convert this to a sum:\n",
        "$$ \\nabla_{\\theta} lnp(\\tau') = \\sum_{i=0} \\nabla_{\\theta} ln \\pi_\\theta (a_i \\vert s_i) + \\sum_{i=0} \\nabla_{\\theta} ln p(s_{i+1} | a_i, s_i) \\tag{3} $$\n",
        "\n",
        "Now, the right summation depends only on the environment and is constant in terms of $\\theta$, so we can disregard it completely. Applying $(3)$ to $(2)$, we get:\n",
        "\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau}[( \\sum_{i=0} \\nabla_{\\theta} ln \\pi_\\theta (a_i \\vert s_i) ) \\cdot R] \\tag{4} $$\n",
        "\n",
        "$(4)$ is the acual formula used for policy gradient.\n",
        "\n",
        "### REINFORCE\n",
        "One algorithm that uses the results of policy gradient is REINFORCE, which, essentially, approximates the reward expectation using Monte Carlo. That is, given an agent (neural network that spits out action probabilities given an input state) $\\pi(\\theta)$, plays the game, gathering samples (similar to the PLAY part of the DQN) and uses those (and their associated rewards) for training.\n",
        "\n",
        "One trick to make computations easier is to use the \"casuality trick\", that is, we are safe, for each step $i$ to multiply with the rewards starting from this state and not the whole $R$, which could make our training more stable. That is, our loss function becomes:\n",
        "$$ L(\\theta) = -\\frac {1}{N} \\sum_{k=1}^{N} \\sum_{i=0} (ln \\pi_\\theta (a_i \\vert s_i) R_i) \\tag{5} $$\n",
        "\n",
        "Some remarks for $(5)$:\n",
        "* $R_i$ is still a constant, but this time is the reward starting from step $i$: $$ R_i = \\sum_{t=i} \\gamma_{t-i} r_{t} $$\n",
        "* By using $(5)$ as loss function in PyTorch, essntially, during the backwards process, it actually optimizes $(4)$. I indroduce the negative sign at the beggining to transform this to a gradient descent optimization problem.\n",
        "\n",
        "To sum up, we do something similar to the online/offline DQL. We can have multiple epochs. On each epoch, we play the game $N$ times with the model (agent), which doesn't update for the duration of the epoch. On each step, we sample an action from its policy output, as well as its probability $p(a_i)$. We can have our model return logits that we can pass through a softmax layer. Then, we compute $lnp(a_i)$ and store it.\n",
        "\n",
        "When we are done, we compute $R_t$ backwards, ie $ R_t = r_t + \\gamma R_{t+1} $ and multiply it with $lnp(a_i)$. Summing all these terms, we get the loss function for one run. Averaging thse values across all runs, we get our final loss function $(5)$. We can then return gradients and update our model.\n",
        "\n",
        "On a side note, in our case, we can do batch gradient descent. However, if we had a complex game with a lot of data, we could use mini-batch GD with say batch size of 256. Each batch can be an independent \"epoch\" and average across the current batch.\n",
        "\n",
        "Let me implement reinforce below:\n"
      ],
      "metadata": {
        "id": "85n8VTG7CpE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PolicyAgent(nn.Module): # policy\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyAgent, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # logits\n",
        "\n",
        "        return F.softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "cdpYwuxoCrCA"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_R_t(rewards, gamma):\n",
        "    R_t = len(rewards) * [0] # no need for backprop (is a constant)\n",
        "\n",
        "    R_t[-1] = rewards[-1]\n",
        "\n",
        "    for i in range(len(rewards) - 2, -1, -1):\n",
        "        R_t[i] = rewards[i] + gamma * (R_t[i + 1])\n",
        "\n",
        "    return R_t\n",
        "\n",
        "def sample_action(model, observation): # sample an action from the policy distribution for state\n",
        "    action_probs = model(observation) # (action_space)\n",
        "\n",
        "    action_dist = torch.distributions.Categorical(action_probs)\n",
        "\n",
        "    action = action_dist.sample()\n",
        "    action_prob_log = action_dist.log_prob(action)\n",
        "\n",
        "    return action.item(), action_prob_log\n",
        "\n",
        "def reinforce(env, model, optimizer, num_epochs, runs_per_epoch, gamma, device):\n",
        "    for epoch in range(num_epochs):\n",
        "        outputs = []\n",
        "        mean_rewards = [] # for logging\n",
        "\n",
        "        # PLAY\n",
        "        for _ in range(runs_per_epoch):\n",
        "            rewards = []\n",
        "            action_prob_logs = []\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            observation, info = env.reset()\n",
        "            observation = torch.tensor(observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "            action, action_prob_log = sample_action(model, observation) # seed action\n",
        "            action_prob_logs.append(action_prob_log)\n",
        "\n",
        "            while True:\n",
        "                observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "                rewards.append(reward)\n",
        "\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "\n",
        "                observation = torch.tensor(observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "                action, action_prob_log = sample_action(model, observation) # careful! not the max!\n",
        "                action_prob_logs.append(action_prob_log)\n",
        "\n",
        "            mean_rewards.append(np.mean(rewards))\n",
        "            R_t = compute_R_t(rewards, gamma)\n",
        "\n",
        "            R_t = torch.tensor(R_t, dtype=torch.float32, device=device)\n",
        "            action_prob_logs = torch.stack(action_prob_logs).to(device)\n",
        "\n",
        "            curr_loss_component = -torch.sum(action_prob_logs * R_t)\n",
        "            outputs.append(curr_loss_component)\n",
        "\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "\n",
        "        outputs = torch.stack(outputs).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = torch.mean(outputs)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}   Avg Loss: {loss.item():.4f}, Avg Reward: {np.mean(mean_rewards):.2f}')\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "_Ey_5E7mVzPg"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "runs_per_epoch = 50\n",
        "gamma = 0.99\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "model = PolicyAgent(obs_state_dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "reinforce(env, model, optimizer, num_epochs, runs_per_epoch, gamma, device)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xDIje7XKhCuf",
        "outputId": "d3687254-77f1-4fde-cffc-bd482ba9fd57"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1   Avg Loss: -11750.0098, Avg Reward: -1.88\n",
            "Epoch 2   Avg Loss: -12404.7490, Avg Reward: -1.98\n",
            "Epoch 3   Avg Loss: -12305.9365, Avg Reward: -2.12\n",
            "Epoch 4   Avg Loss: -10902.0840, Avg Reward: -1.85\n",
            "Epoch 5   Avg Loss: -10650.5029, Avg Reward: -1.91\n",
            "Epoch 6   Avg Loss: -12607.1328, Avg Reward: -2.15\n",
            "Epoch 7   Avg Loss: -10442.2285, Avg Reward: -1.80\n",
            "Epoch 8   Avg Loss: -12437.5527, Avg Reward: -2.06\n",
            "Epoch 9   Avg Loss: -12936.0215, Avg Reward: -2.07\n",
            "Epoch 10   Avg Loss: -9590.5879, Avg Reward: -1.63\n",
            "Epoch 11   Avg Loss: -11193.7539, Avg Reward: -1.78\n",
            "Epoch 12   Avg Loss: -12170.5967, Avg Reward: -2.02\n",
            "Epoch 13   Avg Loss: -13207.8389, Avg Reward: -2.03\n",
            "Epoch 14   Avg Loss: -10097.1162, Avg Reward: -1.72\n",
            "Epoch 15   Avg Loss: -9052.3564, Avg Reward: -1.49\n",
            "Epoch 16   Avg Loss: -11505.5078, Avg Reward: -1.65\n",
            "Epoch 17   Avg Loss: -11414.4688, Avg Reward: -1.75\n",
            "Epoch 18   Avg Loss: -11060.4004, Avg Reward: -1.77\n",
            "Epoch 19   Avg Loss: -10181.6338, Avg Reward: -1.74\n",
            "Epoch 20   Avg Loss: -11486.6201, Avg Reward: -1.75\n",
            "Epoch 21   Avg Loss: -10212.1650, Avg Reward: -1.59\n",
            "Epoch 22   Avg Loss: -9979.4277, Avg Reward: -1.69\n",
            "Epoch 23   Avg Loss: -11129.0225, Avg Reward: -1.68\n",
            "Epoch 24   Avg Loss: -9603.4766, Avg Reward: -1.62\n",
            "Epoch 25   Avg Loss: -9786.7627, Avg Reward: -1.62\n",
            "Epoch 26   Avg Loss: -9484.8740, Avg Reward: -1.49\n",
            "Epoch 27   Avg Loss: -9569.8047, Avg Reward: -1.48\n",
            "Epoch 28   Avg Loss: -9727.2666, Avg Reward: -1.48\n",
            "Epoch 29   Avg Loss: -10091.4219, Avg Reward: -1.41\n",
            "Epoch 30   Avg Loss: -10231.0703, Avg Reward: -1.43\n",
            "Epoch 31   Avg Loss: -9463.4727, Avg Reward: -1.39\n",
            "Epoch 32   Avg Loss: -7786.8833, Avg Reward: -1.28\n",
            "Epoch 33   Avg Loss: -10297.9199, Avg Reward: -1.55\n",
            "Epoch 34   Avg Loss: -9224.1426, Avg Reward: -1.49\n",
            "Epoch 35   Avg Loss: -9120.0498, Avg Reward: -1.43\n",
            "Epoch 36   Avg Loss: -10437.3184, Avg Reward: -1.48\n",
            "Epoch 37   Avg Loss: -9795.0625, Avg Reward: -1.46\n",
            "Epoch 38   Avg Loss: -7658.4624, Avg Reward: -1.25\n",
            "Epoch 39   Avg Loss: -8479.2227, Avg Reward: -1.34\n",
            "Epoch 40   Avg Loss: -8367.7891, Avg Reward: -1.37\n",
            "Epoch 41   Avg Loss: -9586.1660, Avg Reward: -1.42\n",
            "Epoch 42   Avg Loss: -8521.3730, Avg Reward: -1.31\n",
            "Epoch 43   Avg Loss: -8918.0205, Avg Reward: -1.37\n",
            "Epoch 44   Avg Loss: -8272.1953, Avg Reward: -1.38\n",
            "Epoch 45   Avg Loss: -10873.4629, Avg Reward: -1.40\n",
            "Epoch 46   Avg Loss: -9135.5146, Avg Reward: -1.35\n",
            "Epoch 47   Avg Loss: -9087.8193, Avg Reward: -1.31\n",
            "Epoch 48   Avg Loss: -10283.1035, Avg Reward: -1.44\n",
            "Epoch 49   Avg Loss: -8998.3682, Avg Reward: -1.31\n",
            "Epoch 50   Avg Loss: -9162.5811, Avg Reward: -1.24\n",
            "Epoch 51   Avg Loss: -10655.6514, Avg Reward: -1.49\n",
            "Epoch 52   Avg Loss: -7822.5205, Avg Reward: -1.27\n",
            "Epoch 53   Avg Loss: -9562.0391, Avg Reward: -1.46\n",
            "Epoch 54   Avg Loss: -8669.5361, Avg Reward: -1.34\n",
            "Epoch 55   Avg Loss: -8730.9502, Avg Reward: -1.22\n",
            "Epoch 56   Avg Loss: -8495.5625, Avg Reward: -1.21\n",
            "Epoch 57   Avg Loss: -8581.9189, Avg Reward: -1.34\n",
            "Epoch 58   Avg Loss: -8934.5332, Avg Reward: -1.28\n",
            "Epoch 59   Avg Loss: -9141.5215, Avg Reward: -1.37\n",
            "Epoch 60   Avg Loss: -9911.9736, Avg Reward: -1.40\n",
            "Epoch 61   Avg Loss: -8743.0908, Avg Reward: -1.35\n",
            "Epoch 62   Avg Loss: -8278.7539, Avg Reward: -1.27\n",
            "Epoch 63   Avg Loss: -10003.0254, Avg Reward: -1.35\n",
            "Epoch 64   Avg Loss: -8349.5713, Avg Reward: -1.23\n",
            "Epoch 65   Avg Loss: -8511.4658, Avg Reward: -1.21\n",
            "Epoch 66   Avg Loss: -9736.0391, Avg Reward: -1.30\n",
            "Epoch 67   Avg Loss: -9258.9443, Avg Reward: -1.22\n",
            "Epoch 68   Avg Loss: -8184.3677, Avg Reward: -1.19\n",
            "Epoch 69   Avg Loss: -8187.5269, Avg Reward: -1.24\n",
            "Epoch 70   Avg Loss: -8615.0068, Avg Reward: -1.27\n",
            "Epoch 71   Avg Loss: -8344.8672, Avg Reward: -1.16\n",
            "Epoch 72   Avg Loss: -9242.3555, Avg Reward: -1.21\n",
            "Epoch 73   Avg Loss: -8626.8799, Avg Reward: -1.16\n",
            "Epoch 74   Avg Loss: -8083.2427, Avg Reward: -1.13\n",
            "Epoch 75   Avg Loss: -8607.9297, Avg Reward: -1.17\n",
            "Epoch 76   Avg Loss: -9908.7549, Avg Reward: -1.43\n",
            "Epoch 77   Avg Loss: -7563.3623, Avg Reward: -1.07\n",
            "Epoch 78   Avg Loss: -7853.7100, Avg Reward: -1.15\n",
            "Epoch 79   Avg Loss: -7874.0649, Avg Reward: -1.09\n",
            "Epoch 80   Avg Loss: -7738.0000, Avg Reward: -1.00\n",
            "Epoch 81   Avg Loss: -8513.4004, Avg Reward: -1.09\n",
            "Epoch 82   Avg Loss: -8761.3066, Avg Reward: -1.19\n",
            "Epoch 83   Avg Loss: -7442.1533, Avg Reward: -0.98\n",
            "Epoch 84   Avg Loss: -7200.0586, Avg Reward: -1.01\n",
            "Epoch 85   Avg Loss: -7709.8799, Avg Reward: -1.03\n",
            "Epoch 86   Avg Loss: -8116.5874, Avg Reward: -1.01\n",
            "Epoch 87   Avg Loss: -7196.3394, Avg Reward: -0.93\n",
            "Epoch 88   Avg Loss: -6790.2090, Avg Reward: -0.87\n",
            "Epoch 89   Avg Loss: -7244.5864, Avg Reward: -0.91\n",
            "Epoch 90   Avg Loss: -7203.3638, Avg Reward: -0.92\n",
            "Epoch 91   Avg Loss: -7422.2812, Avg Reward: -0.96\n",
            "Epoch 92   Avg Loss: -7338.2344, Avg Reward: -0.83\n",
            "Epoch 93   Avg Loss: -7456.8677, Avg Reward: -0.80\n",
            "Epoch 94   Avg Loss: -7277.4795, Avg Reward: -0.71\n",
            "Epoch 95   Avg Loss: -7267.2417, Avg Reward: -0.76\n",
            "Epoch 96   Avg Loss: -7778.6567, Avg Reward: -0.82\n",
            "Epoch 97   Avg Loss: -6175.6748, Avg Reward: -0.63\n",
            "Epoch 98   Avg Loss: -7196.3311, Avg Reward: -0.70\n",
            "Epoch 99   Avg Loss: -7536.3184, Avg Reward: -0.59\n",
            "Epoch 100   Avg Loss: -6529.4092, Avg Reward: -0.62\n",
            "Epoch 101   Avg Loss: -6813.3711, Avg Reward: -0.67\n",
            "Epoch 102   Avg Loss: -6297.9844, Avg Reward: -0.59\n",
            "Epoch 103   Avg Loss: -6324.6519, Avg Reward: -0.58\n",
            "Epoch 104   Avg Loss: -6019.4844, Avg Reward: -0.58\n",
            "Epoch 105   Avg Loss: -6829.9023, Avg Reward: -0.62\n",
            "Epoch 106   Avg Loss: -4935.2622, Avg Reward: -0.45\n",
            "Epoch 107   Avg Loss: -5998.2524, Avg Reward: -0.54\n",
            "Epoch 108   Avg Loss: -6511.9844, Avg Reward: -0.45\n",
            "Epoch 109   Avg Loss: -5655.9194, Avg Reward: -0.46\n",
            "Epoch 110   Avg Loss: -5771.0020, Avg Reward: -0.53\n",
            "Epoch 111   Avg Loss: -5013.2051, Avg Reward: -0.45\n",
            "Epoch 112   Avg Loss: -4822.2109, Avg Reward: -0.39\n",
            "Epoch 113   Avg Loss: -5298.9531, Avg Reward: -0.32\n",
            "Epoch 114   Avg Loss: -5390.3735, Avg Reward: -0.34\n",
            "Epoch 115   Avg Loss: -5549.1455, Avg Reward: -0.30\n",
            "Epoch 116   Avg Loss: -6896.3848, Avg Reward: -0.36\n",
            "Epoch 117   Avg Loss: -5293.2788, Avg Reward: -0.35\n",
            "Epoch 118   Avg Loss: -5287.9668, Avg Reward: -0.31\n",
            "Epoch 119   Avg Loss: -5097.7832, Avg Reward: -0.27\n",
            "Epoch 120   Avg Loss: -5645.1118, Avg Reward: -0.30\n",
            "Epoch 121   Avg Loss: -6585.4663, Avg Reward: -0.34\n",
            "Epoch 122   Avg Loss: -7907.5737, Avg Reward: -0.32\n",
            "Epoch 123   Avg Loss: -6740.0264, Avg Reward: -0.30\n",
            "Epoch 124   Avg Loss: -7859.7852, Avg Reward: -0.30\n",
            "Epoch 125   Avg Loss: -5344.4536, Avg Reward: -0.25\n",
            "Epoch 126   Avg Loss: -4537.7026, Avg Reward: -0.20\n",
            "Epoch 127   Avg Loss: -5211.7329, Avg Reward: -0.20\n",
            "Epoch 128   Avg Loss: -4399.3745, Avg Reward: -0.20\n",
            "Epoch 129   Avg Loss: -5097.1050, Avg Reward: -0.24\n",
            "Epoch 130   Avg Loss: -5449.6582, Avg Reward: -0.11\n",
            "Epoch 131   Avg Loss: -4881.1729, Avg Reward: -0.13\n",
            "Epoch 132   Avg Loss: -5403.4170, Avg Reward: -0.14\n",
            "Epoch 133   Avg Loss: -3161.3406, Avg Reward: -0.08\n",
            "Epoch 134   Avg Loss: -4081.9863, Avg Reward: -0.16\n",
            "Epoch 135   Avg Loss: -3059.8491, Avg Reward: -0.10\n",
            "Epoch 136   Avg Loss: -4046.4382, Avg Reward: -0.06\n",
            "Epoch 137   Avg Loss: -4146.6191, Avg Reward: -0.11\n",
            "Epoch 138   Avg Loss: -2315.9646, Avg Reward: -0.06\n",
            "Epoch 139   Avg Loss: -2005.6942, Avg Reward: -0.01\n",
            "Epoch 140   Avg Loss: -3387.0391, Avg Reward: -0.09\n",
            "Epoch 141   Avg Loss: -4471.8462, Avg Reward: -0.12\n",
            "Epoch 142   Avg Loss: -4482.0532, Avg Reward: -0.08\n",
            "Epoch 143   Avg Loss: -4924.8872, Avg Reward: -0.08\n",
            "Epoch 144   Avg Loss: -9066.0654, Avg Reward: -0.12\n",
            "Epoch 145   Avg Loss: -6809.0386, Avg Reward: -0.09\n",
            "Epoch 146   Avg Loss: -5352.3882, Avg Reward: -0.07\n",
            "Epoch 147   Avg Loss: -3965.6775, Avg Reward: -0.04\n",
            "Epoch 148   Avg Loss: -4420.6562, Avg Reward: -0.07\n",
            "Epoch 149   Avg Loss: -3872.6904, Avg Reward: -0.04\n",
            "Epoch 150   Avg Loss: -3010.3459, Avg Reward: -0.02\n",
            "Epoch 151   Avg Loss: -6802.2817, Avg Reward: -0.11\n",
            "Epoch 152   Avg Loss: -2439.5142, Avg Reward: -0.02\n",
            "Epoch 153   Avg Loss: -4709.9644, Avg Reward: -0.00\n",
            "Epoch 154   Avg Loss: -4066.5908, Avg Reward: -0.02\n",
            "Epoch 155   Avg Loss: -4772.9629, Avg Reward: -0.03\n",
            "Epoch 156   Avg Loss: -5136.6777, Avg Reward: -0.04\n",
            "Epoch 157   Avg Loss: -5289.0361, Avg Reward: -0.02\n",
            "Epoch 158   Avg Loss: -4476.8789, Avg Reward: -0.07\n",
            "Epoch 159   Avg Loss: -3126.6978, Avg Reward: -0.04\n",
            "Epoch 160   Avg Loss: -2608.4385, Avg Reward: 0.05\n",
            "Epoch 161   Avg Loss: -2782.4819, Avg Reward: 0.03\n",
            "Epoch 162   Avg Loss: -5030.8140, Avg Reward: -0.03\n",
            "Epoch 163   Avg Loss: -4256.4746, Avg Reward: 0.02\n",
            "Epoch 164   Avg Loss: -4539.0088, Avg Reward: -0.01\n",
            "Epoch 165   Avg Loss: -4817.3594, Avg Reward: -0.03\n",
            "Epoch 166   Avg Loss: -5202.9629, Avg Reward: -0.02\n",
            "Epoch 167   Avg Loss: -5833.7139, Avg Reward: -0.04\n",
            "Epoch 168   Avg Loss: -5648.4570, Avg Reward: -0.01\n",
            "Epoch 169   Avg Loss: -4072.1699, Avg Reward: 0.00\n",
            "Epoch 170   Avg Loss: -2180.9832, Avg Reward: 0.02\n",
            "Epoch 171   Avg Loss: -4151.6602, Avg Reward: -0.02\n",
            "Epoch 172   Avg Loss: -3651.9104, Avg Reward: -0.01\n",
            "Epoch 173   Avg Loss: -653.0629, Avg Reward: 0.09\n",
            "Epoch 174   Avg Loss: -1239.9360, Avg Reward: 0.08\n",
            "Epoch 175   Avg Loss: -654.4894, Avg Reward: 0.11\n",
            "Epoch 176   Avg Loss: -426.5716, Avg Reward: 0.09\n",
            "Epoch 177   Avg Loss: -715.1852, Avg Reward: 0.06\n",
            "Epoch 178   Avg Loss: -1850.1707, Avg Reward: 0.06\n",
            "Epoch 179   Avg Loss: -889.9044, Avg Reward: 0.05\n",
            "Epoch 180   Avg Loss: -1311.8140, Avg Reward: 0.03\n",
            "Epoch 181   Avg Loss: -1495.4200, Avg Reward: 0.05\n",
            "Epoch 182   Avg Loss: -765.1473, Avg Reward: 0.04\n",
            "Epoch 183   Avg Loss: -33.9158, Avg Reward: 0.07\n",
            "Epoch 184   Avg Loss: -1049.4666, Avg Reward: 0.05\n",
            "Epoch 185   Avg Loss: 1575.7823, Avg Reward: 0.09\n",
            "Epoch 186   Avg Loss: 1520.0767, Avg Reward: 0.08\n",
            "Epoch 187   Avg Loss: 794.0991, Avg Reward: 0.06\n",
            "Epoch 188   Avg Loss: 1164.4813, Avg Reward: 0.08\n",
            "Epoch 189   Avg Loss: -606.9198, Avg Reward: 0.04\n",
            "Epoch 190   Avg Loss: 2820.7302, Avg Reward: 0.11\n",
            "Epoch 191   Avg Loss: 1553.4272, Avg Reward: 0.07\n",
            "Epoch 192   Avg Loss: 3052.6091, Avg Reward: 0.08\n",
            "Epoch 193   Avg Loss: 3070.4204, Avg Reward: 0.11\n",
            "Epoch 194   Avg Loss: 1825.7706, Avg Reward: 0.09\n",
            "Epoch 195   Avg Loss: 3322.5366, Avg Reward: 0.10\n",
            "Epoch 196   Avg Loss: 4349.3794, Avg Reward: 0.12\n",
            "Epoch 197   Avg Loss: 2944.8665, Avg Reward: 0.09\n",
            "Epoch 198   Avg Loss: 4696.3936, Avg Reward: 0.12\n",
            "Epoch 199   Avg Loss: 2629.5637, Avg Reward: 0.09\n",
            "Epoch 200   Avg Loss: 4446.3535, Avg Reward: 0.12\n",
            "Epoch 201   Avg Loss: 3454.0186, Avg Reward: 0.10\n",
            "Epoch 202   Avg Loss: 3194.7900, Avg Reward: 0.09\n",
            "Epoch 203   Avg Loss: 3840.1575, Avg Reward: 0.11\n",
            "Epoch 204   Avg Loss: 5909.7573, Avg Reward: 0.12\n",
            "Epoch 205   Avg Loss: 5452.7192, Avg Reward: 0.12\n",
            "Epoch 206   Avg Loss: 5074.1245, Avg Reward: 0.12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-99562a026122>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-9c70dd3b1c0c>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, model, optimizer, num_epochs, runs_per_epoch, gamma, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (obs_state_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# careful! not the max!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0maction_prob_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-9c70dd3b1c0c>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(model, observation)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sample an action from the policy distribution for state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (action_space)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-76ca1c285f9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1hr 6min execution above"
      ],
      "metadata": {
        "id": "h2fdfPkt3RBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xm48Wx3zh7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor-Critic\n",
        "REINFORCE baseline is a more advanced variation of REINFORCE, which allows us to reduce the variation introduced by the $R_i$ future reward at step $i$, as this value can vary significantly across trajectories. A baseline can be any function $B(s)$ that accepts as input the current state $s$ and returns a number. The policy gradient can then be written as:\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau}[ \\sum_{i=0} (\\nabla_{\\theta} ln \\pi_\\theta (a_i \\vert s_i)  \\cdot (R_i - B(s_i)) )] $$\n",
        "\n",
        "The idea behind this is that, if we can find a good enough baseline $B$, we can reduce the variance of different trajectories and have smooth training. However, like with REINFORCE, we have to play the whole epoch before returning gradients and updating our model, which is not always the best.\n",
        "\n",
        "Here comes the Actor-Critic (AC) framework. We can replace this $R_i - B(s_i)$ term with a Q-value estimation for the current state $s_i$ and action $a_i$. This can take the form of a 2nd agent, the critic, which work in a pretty similar way to a DQN. To summarize, for the plain AC, we get:\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau}[ \\sum_{i=0} (\\nabla_{\\theta} ln \\pi_\\theta (a_i \\vert s_i)  \\cdot Q(s_i, a_i) )] $$\n",
        "\n",
        "There are advanced variations of the plain AC, like the Advantage Actor-Critic (A2C) that incorporates the advantage function as the baseline:\n",
        "$$ A(s_i, a_i) = Q(s_i, a_i) - V(s_i) = r_i + \\gamma V(s_{i+1}) - V(s_i)  $$\n",
        "which utilizes a value function that maps a state $s$ to its estimated value.\n",
        "\n",
        "The critic is also needs to be updated and it works like in DQL. Our goal is to have it return accurate estimations for input states. This can be done using the TD loss squared, which is a very similar MSQE to the one used in QL:\n",
        "$$ J_c(w) = \\lVert r_i + \\gamma V(s_{i+1}; w) - V(s_i; w) \\rVert^2 $$\n",
        "\n",
        "We can update both the actor and the critic on each step of our episodes, but we need to be careful to detach the estimations of the critic while updating the actor. This also means, the loss for the actor is quite simple too:\n",
        "$$ J_a(\\theta) = (r_i + \\gamma V(s_{i+1}) - V(s_{i})) \\cdot \\nabla_{\\theta} ln \\pi_\\theta (a_i \\vert s_i) $$\n",
        "\n",
        "Below I am implementing the A2C to use the 1-step TD (there exist variations with multiple steps as well).\n",
        "\n",
        "Here is an overview of the A2C algorithm:\n",
        "* For each step on every episode:\n",
        "  1. Sample the next action $a$ using the actor.\n",
        "  2. Compute the loss for the actor $J_a(\\theta)$ and update the actor. Make sure to detach the tensors that involved the critic.\n",
        "  3. Compute the loss for the critic $J_c(w)$ and update the critic."
      ],
      "metadata": {
        "id": "FokLTrVCy1Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action(model, observation): # sample an action from the policy distribution for state\n",
        "    action_probs = model(observation) # (action_space)\n",
        "\n",
        "    action_dist = torch.distributions.Categorical(action_probs)\n",
        "\n",
        "    action = action_dist.sample()\n",
        "    action_prob_log = action_dist.log_prob(action)\n",
        "\n",
        "    return action.item(), action_prob_log\n",
        "\n",
        "def a2c(env, actor, optimizer_a, critic, optimizer_c, num_episodes, gamma, device, supress_output=False):\n",
        "    critic_criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        rewards_sum, num_steps = 0.0, 0\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        observation = torch.tensor(observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "        while True:\n",
        "            # 1. Sample from actor\n",
        "            action, action_prob_log = sample_action(actor, observation) # s_t -> a_t\n",
        "\n",
        "            next_observation, reward, terminated, truncated, info = env.step(action) # s_{t+1}\n",
        "            next_observation = torch.tensor(next_observation, dtype=torch.float32, device=device) # (obs_state_dim)\n",
        "\n",
        "            curr_value = critic(observation) # cache values\n",
        "            next_value = critic(next_observation)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                next_value = torch.tensor([0.0], device=device) # no value if terminated, like in DQL\n",
        "\n",
        "            # 2. Compute loss and update actor\n",
        "            value_component = reward + gamma * next_value.detach() - curr_value.detach()\n",
        "            actor_loss = -action_prob_log * value_component\n",
        "\n",
        "            optimizer_a.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            optimizer_a.step()\n",
        "\n",
        "            # 3. Compute loss and update critic\n",
        "            next_pred = reward + gamma * next_value\n",
        "            critic_loss = critic_criterion(next_pred, critic(observation))\n",
        "\n",
        "            optimizer_c.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            optimizer_c.step()\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "\n",
        "            observation = next_observation # update the observation\n",
        "            rewards_sum += reward\n",
        "            num_steps += 1\n",
        "\n",
        "        if not supress_output or (episode + 1) % 50 == 0:\n",
        "            print(f'Episode {episode+1}   Avg Reward: {rewards_sum / num_steps:.2f}')\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "_Sa0saK8zFqD"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ValueDNN(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueDNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tekv3SOfIOXC"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 300\n",
        "gamma = 0.99\n",
        "lr_a = 0.001\n",
        "lr_c = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "actor = PolicyAgent(obs_state_dim, num_actions).to(device)\n",
        "optimizer_a = torch.optim.Adam(model.parameters(), lr=lr_a)\n",
        "\n",
        "critic = ValueDNN(obs_state_dim).to(device)\n",
        "optimizer_c = torch.optim.Adam(model.parameters(), lr=lr_c)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "a2c(env, actor, optimizer_a, critic, optimizer_c, num_episodes, gamma, device)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvZGIxAhN2dw",
        "outputId": "41f2b21b-e0ce-4b17-cc49-a43d153035ef"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1   Avg Reward: 0.04\n",
            "Episode 2   Avg Reward: -0.86\n",
            "Episode 3   Avg Reward: -0.34\n",
            "Episode 4   Avg Reward: -0.39\n",
            "Episode 5   Avg Reward: -1.14\n",
            "Episode 6   Avg Reward: -0.67\n",
            "Episode 7   Avg Reward: 0.33\n",
            "Episode 8   Avg Reward: -0.08\n",
            "Episode 9   Avg Reward: -2.16\n",
            "Episode 10   Avg Reward: -0.96\n",
            "Episode 11   Avg Reward: -1.57\n",
            "Episode 12   Avg Reward: -0.04\n",
            "Episode 13   Avg Reward: -1.18\n",
            "Episode 14   Avg Reward: -1.58\n",
            "Episode 15   Avg Reward: -2.88\n",
            "Episode 16   Avg Reward: 0.12\n",
            "Episode 17   Avg Reward: 0.38\n",
            "Episode 18   Avg Reward: -3.31\n",
            "Episode 19   Avg Reward: 0.04\n",
            "Episode 20   Avg Reward: 0.43\n",
            "Episode 21   Avg Reward: 0.02\n",
            "Episode 22   Avg Reward: -0.03\n",
            "Episode 23   Avg Reward: -1.53\n",
            "Episode 24   Avg Reward: -0.39\n",
            "Episode 25   Avg Reward: -0.63\n",
            "Episode 26   Avg Reward: -0.33\n",
            "Episode 27   Avg Reward: -0.50\n",
            "Episode 28   Avg Reward: -2.34\n",
            "Episode 29   Avg Reward: -0.93\n",
            "Episode 30   Avg Reward: -0.08\n",
            "Episode 31   Avg Reward: 0.42\n",
            "Episode 32   Avg Reward: -1.22\n",
            "Episode 33   Avg Reward: 0.77\n",
            "Episode 34   Avg Reward: -0.40\n",
            "Episode 35   Avg Reward: -0.50\n",
            "Episode 36   Avg Reward: 0.05\n",
            "Episode 37   Avg Reward: -0.89\n",
            "Episode 38   Avg Reward: -0.27\n",
            "Episode 39   Avg Reward: 1.22\n",
            "Episode 40   Avg Reward: -1.95\n",
            "Episode 41   Avg Reward: -1.03\n",
            "Episode 42   Avg Reward: -1.32\n",
            "Episode 43   Avg Reward: 0.14\n",
            "Episode 44   Avg Reward: -0.21\n",
            "Episode 45   Avg Reward: 0.14\n",
            "Episode 46   Avg Reward: -3.30\n",
            "Episode 47   Avg Reward: -0.79\n",
            "Episode 48   Avg Reward: -2.20\n",
            "Episode 49   Avg Reward: 1.20\n",
            "Episode 50   Avg Reward: -2.08\n",
            "Episode 51   Avg Reward: -0.17\n",
            "Episode 52   Avg Reward: -2.41\n",
            "Episode 53   Avg Reward: -1.28\n",
            "Episode 54   Avg Reward: -2.03\n",
            "Episode 55   Avg Reward: -0.64\n",
            "Episode 56   Avg Reward: -0.83\n",
            "Episode 57   Avg Reward: -1.74\n",
            "Episode 58   Avg Reward: -0.23\n",
            "Episode 59   Avg Reward: -3.26\n",
            "Episode 60   Avg Reward: -2.03\n",
            "Episode 61   Avg Reward: -1.17\n",
            "Episode 62   Avg Reward: -2.53\n",
            "Episode 63   Avg Reward: -1.45\n",
            "Episode 64   Avg Reward: -0.95\n",
            "Episode 65   Avg Reward: 0.22\n",
            "Episode 66   Avg Reward: -1.62\n",
            "Episode 67   Avg Reward: 0.08\n",
            "Episode 68   Avg Reward: -1.36\n",
            "Episode 69   Avg Reward: -2.33\n",
            "Episode 70   Avg Reward: -2.05\n",
            "Episode 71   Avg Reward: -0.73\n",
            "Episode 72   Avg Reward: -2.58\n",
            "Episode 73   Avg Reward: -0.97\n",
            "Episode 74   Avg Reward: -1.16\n",
            "Episode 75   Avg Reward: -4.28\n",
            "Episode 76   Avg Reward: -0.46\n",
            "Episode 77   Avg Reward: -0.03\n",
            "Episode 78   Avg Reward: -1.77\n",
            "Episode 79   Avg Reward: -2.63\n",
            "Episode 80   Avg Reward: -0.20\n",
            "Episode 81   Avg Reward: -1.36\n",
            "Episode 82   Avg Reward: -2.16\n",
            "Episode 83   Avg Reward: -2.24\n",
            "Episode 84   Avg Reward: -1.33\n",
            "Episode 85   Avg Reward: -1.27\n",
            "Episode 86   Avg Reward: 0.39\n",
            "Episode 87   Avg Reward: -2.85\n",
            "Episode 88   Avg Reward: -0.14\n",
            "Episode 89   Avg Reward: 0.10\n",
            "Episode 90   Avg Reward: -1.54\n",
            "Episode 91   Avg Reward: 0.53\n",
            "Episode 92   Avg Reward: -0.52\n",
            "Episode 93   Avg Reward: -0.07\n",
            "Episode 94   Avg Reward: -2.33\n",
            "Episode 95   Avg Reward: 0.08\n",
            "Episode 96   Avg Reward: 0.93\n",
            "Episode 97   Avg Reward: -1.72\n",
            "Episode 98   Avg Reward: 0.83\n",
            "Episode 99   Avg Reward: -0.72\n",
            "Episode 100   Avg Reward: -0.28\n",
            "Episode 101   Avg Reward: -4.37\n",
            "Episode 102   Avg Reward: -0.51\n",
            "Episode 103   Avg Reward: -3.18\n",
            "Episode 104   Avg Reward: -2.99\n",
            "Episode 105   Avg Reward: -0.66\n",
            "Episode 106   Avg Reward: -1.72\n",
            "Episode 107   Avg Reward: -1.26\n",
            "Episode 108   Avg Reward: -0.15\n",
            "Episode 109   Avg Reward: 0.16\n",
            "Episode 110   Avg Reward: -1.85\n",
            "Episode 111   Avg Reward: -0.09\n",
            "Episode 112   Avg Reward: -2.10\n",
            "Episode 113   Avg Reward: -0.73\n",
            "Episode 114   Avg Reward: 0.02\n",
            "Episode 115   Avg Reward: -1.08\n",
            "Episode 116   Avg Reward: -2.70\n",
            "Episode 117   Avg Reward: -0.88\n",
            "Episode 118   Avg Reward: -0.40\n",
            "Episode 119   Avg Reward: 0.68\n",
            "Episode 120   Avg Reward: -0.46\n",
            "Episode 121   Avg Reward: -0.40\n",
            "Episode 122   Avg Reward: 0.09\n",
            "Episode 123   Avg Reward: 0.53\n",
            "Episode 124   Avg Reward: 1.30\n",
            "Episode 125   Avg Reward: 0.38\n",
            "Episode 126   Avg Reward: -1.14\n",
            "Episode 127   Avg Reward: -1.05\n",
            "Episode 128   Avg Reward: -0.13\n",
            "Episode 129   Avg Reward: -0.38\n",
            "Episode 130   Avg Reward: -1.17\n",
            "Episode 131   Avg Reward: 0.26\n",
            "Episode 132   Avg Reward: -0.51\n",
            "Episode 133   Avg Reward: -1.65\n",
            "Episode 134   Avg Reward: 0.05\n",
            "Episode 135   Avg Reward: -1.35\n",
            "Episode 136   Avg Reward: -1.10\n",
            "Episode 137   Avg Reward: -0.32\n",
            "Episode 138   Avg Reward: -4.12\n",
            "Episode 139   Avg Reward: 0.74\n",
            "Episode 140   Avg Reward: -0.31\n",
            "Episode 141   Avg Reward: 0.30\n",
            "Episode 142   Avg Reward: -1.98\n",
            "Episode 143   Avg Reward: -2.57\n",
            "Episode 144   Avg Reward: 0.23\n",
            "Episode 145   Avg Reward: 0.06\n",
            "Episode 146   Avg Reward: -0.92\n",
            "Episode 147   Avg Reward: 0.48\n",
            "Episode 148   Avg Reward: -2.52\n",
            "Episode 149   Avg Reward: -3.26\n",
            "Episode 150   Avg Reward: -2.60\n",
            "Episode 151   Avg Reward: -1.26\n",
            "Episode 152   Avg Reward: -3.18\n",
            "Episode 153   Avg Reward: -0.55\n",
            "Episode 154   Avg Reward: 0.31\n",
            "Episode 155   Avg Reward: -3.15\n",
            "Episode 156   Avg Reward: -1.65\n",
            "Episode 157   Avg Reward: -0.16\n",
            "Episode 158   Avg Reward: 0.08\n",
            "Episode 159   Avg Reward: 0.23\n",
            "Episode 160   Avg Reward: -3.01\n",
            "Episode 161   Avg Reward: -0.34\n",
            "Episode 162   Avg Reward: -0.04\n",
            "Episode 163   Avg Reward: -1.56\n",
            "Episode 164   Avg Reward: -2.28\n",
            "Episode 165   Avg Reward: 0.25\n",
            "Episode 166   Avg Reward: -0.02\n",
            "Episode 167   Avg Reward: -0.17\n",
            "Episode 168   Avg Reward: 0.42\n",
            "Episode 169   Avg Reward: -0.04\n",
            "Episode 170   Avg Reward: 0.25\n",
            "Episode 171   Avg Reward: 0.51\n",
            "Episode 172   Avg Reward: 1.13\n",
            "Episode 173   Avg Reward: -3.19\n",
            "Episode 174   Avg Reward: -0.44\n",
            "Episode 175   Avg Reward: -2.85\n",
            "Episode 176   Avg Reward: -2.27\n",
            "Episode 177   Avg Reward: -0.34\n",
            "Episode 178   Avg Reward: -1.64\n",
            "Episode 179   Avg Reward: -0.36\n",
            "Episode 180   Avg Reward: -0.80\n",
            "Episode 181   Avg Reward: 0.84\n",
            "Episode 182   Avg Reward: -1.72\n",
            "Episode 183   Avg Reward: -0.48\n",
            "Episode 184   Avg Reward: 0.02\n",
            "Episode 185   Avg Reward: -4.51\n",
            "Episode 186   Avg Reward: -0.51\n",
            "Episode 187   Avg Reward: -0.15\n",
            "Episode 188   Avg Reward: -1.13\n",
            "Episode 189   Avg Reward: -0.77\n",
            "Episode 190   Avg Reward: -1.80\n",
            "Episode 191   Avg Reward: 0.54\n",
            "Episode 192   Avg Reward: 0.62\n",
            "Episode 193   Avg Reward: -1.03\n",
            "Episode 194   Avg Reward: -1.08\n",
            "Episode 195   Avg Reward: 1.29\n",
            "Episode 196   Avg Reward: -0.60\n",
            "Episode 197   Avg Reward: -1.91\n",
            "Episode 198   Avg Reward: -0.22\n",
            "Episode 199   Avg Reward: -0.57\n",
            "Episode 200   Avg Reward: 0.14\n",
            "Episode 201   Avg Reward: -0.05\n",
            "Episode 202   Avg Reward: -0.24\n",
            "Episode 203   Avg Reward: -0.44\n",
            "Episode 204   Avg Reward: -1.64\n",
            "Episode 205   Avg Reward: 0.71\n",
            "Episode 206   Avg Reward: -0.84\n",
            "Episode 207   Avg Reward: -1.80\n",
            "Episode 208   Avg Reward: -2.56\n",
            "Episode 209   Avg Reward: 0.32\n",
            "Episode 210   Avg Reward: 0.05\n",
            "Episode 211   Avg Reward: -3.78\n",
            "Episode 212   Avg Reward: -1.55\n",
            "Episode 213   Avg Reward: 0.05\n",
            "Episode 214   Avg Reward: -1.68\n",
            "Episode 215   Avg Reward: 0.09\n",
            "Episode 216   Avg Reward: -0.12\n",
            "Episode 217   Avg Reward: 1.52\n",
            "Episode 218   Avg Reward: 0.02\n",
            "Episode 219   Avg Reward: -3.31\n",
            "Episode 220   Avg Reward: -3.29\n",
            "Episode 221   Avg Reward: -1.87\n",
            "Episode 222   Avg Reward: -1.61\n",
            "Episode 223   Avg Reward: -1.23\n",
            "Episode 224   Avg Reward: -2.25\n",
            "Episode 225   Avg Reward: 0.02\n",
            "Episode 226   Avg Reward: -1.20\n",
            "Episode 227   Avg Reward: -0.26\n",
            "Episode 228   Avg Reward: -1.16\n",
            "Episode 229   Avg Reward: 0.37\n",
            "Episode 230   Avg Reward: -1.86\n",
            "Episode 231   Avg Reward: -2.46\n",
            "Episode 232   Avg Reward: -0.51\n",
            "Episode 233   Avg Reward: -0.09\n",
            "Episode 234   Avg Reward: -3.22\n",
            "Episode 235   Avg Reward: -0.35\n",
            "Episode 236   Avg Reward: 0.06\n",
            "Episode 237   Avg Reward: -0.21\n",
            "Episode 238   Avg Reward: -0.53\n",
            "Episode 239   Avg Reward: -0.50\n",
            "Episode 240   Avg Reward: 0.64\n",
            "Episode 241   Avg Reward: -1.01\n",
            "Episode 242   Avg Reward: -0.03\n",
            "Episode 243   Avg Reward: -1.80\n",
            "Episode 244   Avg Reward: -2.89\n",
            "Episode 245   Avg Reward: -1.60\n",
            "Episode 246   Avg Reward: 0.28\n",
            "Episode 247   Avg Reward: -0.08\n",
            "Episode 248   Avg Reward: -1.27\n",
            "Episode 249   Avg Reward: -1.42\n",
            "Episode 250   Avg Reward: -0.34\n",
            "Episode 251   Avg Reward: -2.57\n",
            "Episode 252   Avg Reward: 0.31\n",
            "Episode 253   Avg Reward: -2.37\n",
            "Episode 254   Avg Reward: 0.31\n",
            "Episode 255   Avg Reward: 0.25\n",
            "Episode 256   Avg Reward: -1.51\n",
            "Episode 257   Avg Reward: -2.22\n",
            "Episode 258   Avg Reward: -0.33\n",
            "Episode 259   Avg Reward: 0.17\n",
            "Episode 260   Avg Reward: 0.42\n",
            "Episode 261   Avg Reward: -2.82\n",
            "Episode 262   Avg Reward: -0.83\n",
            "Episode 263   Avg Reward: -0.88\n",
            "Episode 264   Avg Reward: 0.26\n",
            "Episode 265   Avg Reward: 0.31\n",
            "Episode 266   Avg Reward: -1.79\n",
            "Episode 267   Avg Reward: -1.46\n",
            "Episode 268   Avg Reward: -2.71\n",
            "Episode 269   Avg Reward: -2.01\n",
            "Episode 270   Avg Reward: -1.44\n",
            "Episode 271   Avg Reward: -0.10\n",
            "Episode 272   Avg Reward: -0.36\n",
            "Episode 273   Avg Reward: -2.15\n",
            "Episode 274   Avg Reward: -2.88\n",
            "Episode 275   Avg Reward: -1.15\n",
            "Episode 276   Avg Reward: -0.39\n",
            "Episode 277   Avg Reward: -1.32\n",
            "Episode 278   Avg Reward: -0.29\n",
            "Episode 279   Avg Reward: -2.12\n",
            "Episode 280   Avg Reward: -0.12\n",
            "Episode 281   Avg Reward: 0.20\n",
            "Episode 282   Avg Reward: -2.37\n",
            "Episode 283   Avg Reward: -1.81\n",
            "Episode 284   Avg Reward: -0.61\n",
            "Episode 285   Avg Reward: -3.29\n",
            "Episode 286   Avg Reward: 0.45\n",
            "Episode 287   Avg Reward: -2.08\n",
            "Episode 288   Avg Reward: -1.07\n",
            "Episode 289   Avg Reward: -0.36\n",
            "Episode 290   Avg Reward: -0.01\n",
            "Episode 291   Avg Reward: -0.04\n",
            "Episode 292   Avg Reward: -2.32\n",
            "Episode 293   Avg Reward: -2.56\n",
            "Episode 294   Avg Reward: -0.04\n",
            "Episode 295   Avg Reward: -2.39\n",
            "Episode 296   Avg Reward: 0.12\n",
            "Episode 297   Avg Reward: 0.78\n",
            "Episode 298   Avg Reward: -0.63\n",
            "Episode 299   Avg Reward: 0.92\n",
            "Episode 300   Avg Reward: -1.43\n",
            "0:01:42.319833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impressive, we managed to have positive rewards even in the first few episode runs! I run the A2C algorithm once again with more episodes:"
      ],
      "metadata": {
        "id": "vyxIyfOnJo5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 3000\n",
        "gamma = 0.99\n",
        "lr_a = 0.001\n",
        "lr_c = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "obs_state_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "actor = PolicyAgent(obs_state_dim, num_actions).to(device)\n",
        "optimizer_a = torch.optim.Adam(model.parameters(), lr=lr_a)\n",
        "\n",
        "critic = ValueDNN(obs_state_dim).to(device)\n",
        "optimizer_c = torch.optim.Adam(model.parameters(), lr=lr_c)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "a2c(env, actor, optimizer_a, critic, optimizer_c, num_episodes, gamma, device, supress_output=True)\n",
        "\n",
        "end = datetime.now()\n",
        "\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUmAVaciI34J",
        "outputId": "6267a98b-e110-4590-efaf-a33b2ddec9cc"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50   Avg Reward: -0.18\n",
            "Episode 100   Avg Reward: -1.82\n",
            "Episode 150   Avg Reward: 0.06\n",
            "Episode 200   Avg Reward: -1.12\n",
            "Episode 250   Avg Reward: -1.26\n",
            "Episode 300   Avg Reward: -2.14\n",
            "Episode 350   Avg Reward: -2.11\n",
            "Episode 400   Avg Reward: -2.97\n",
            "Episode 450   Avg Reward: -1.70\n",
            "Episode 500   Avg Reward: -1.40\n",
            "Episode 550   Avg Reward: -0.13\n",
            "Episode 600   Avg Reward: -0.24\n",
            "Episode 650   Avg Reward: -0.72\n",
            "Episode 700   Avg Reward: -0.31\n",
            "Episode 750   Avg Reward: 0.03\n",
            "Episode 800   Avg Reward: -1.40\n",
            "Episode 850   Avg Reward: -2.05\n",
            "Episode 900   Avg Reward: 0.30\n",
            "Episode 950   Avg Reward: 0.11\n",
            "Episode 1000   Avg Reward: -2.67\n",
            "Episode 1050   Avg Reward: -3.68\n",
            "Episode 1100   Avg Reward: 0.45\n",
            "Episode 1150   Avg Reward: -0.24\n",
            "Episode 1200   Avg Reward: -0.07\n",
            "Episode 1250   Avg Reward: -1.77\n",
            "Episode 1300   Avg Reward: -1.72\n",
            "Episode 1350   Avg Reward: -1.19\n",
            "Episode 1400   Avg Reward: 0.23\n",
            "Episode 1450   Avg Reward: 0.49\n",
            "Episode 1500   Avg Reward: -0.83\n",
            "Episode 1550   Avg Reward: -3.56\n",
            "Episode 1600   Avg Reward: -0.11\n",
            "Episode 1650   Avg Reward: -4.73\n",
            "Episode 1700   Avg Reward: -0.22\n",
            "Episode 1750   Avg Reward: -3.51\n",
            "Episode 1800   Avg Reward: -1.32\n",
            "Episode 1850   Avg Reward: -0.11\n",
            "Episode 1900   Avg Reward: -3.02\n",
            "Episode 1950   Avg Reward: 0.10\n",
            "Episode 2000   Avg Reward: -0.92\n",
            "Episode 2050   Avg Reward: -0.86\n",
            "Episode 2100   Avg Reward: -0.26\n",
            "Episode 2150   Avg Reward: -2.31\n",
            "Episode 2200   Avg Reward: -0.05\n",
            "Episode 2250   Avg Reward: -0.23\n",
            "Episode 2300   Avg Reward: -3.23\n",
            "Episode 2350   Avg Reward: 0.23\n",
            "Episode 2400   Avg Reward: -0.26\n",
            "Episode 2450   Avg Reward: -0.35\n",
            "Episode 2500   Avg Reward: -1.79\n",
            "Episode 2550   Avg Reward: -2.16\n",
            "Episode 2600   Avg Reward: -0.51\n",
            "Episode 2650   Avg Reward: 0.62\n",
            "Episode 2700   Avg Reward: 0.36\n",
            "Episode 2750   Avg Reward: -0.53\n",
            "Episode 2800   Avg Reward: 0.57\n",
            "Episode 2850   Avg Reward: -0.52\n",
            "Episode 2900   Avg Reward: -1.55\n",
            "Episode 2950   Avg Reward: -0.12\n",
            "Episode 3000   Avg Reward: -0.23\n",
            "0:17:00.078403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rI_UQiuiN6wp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}